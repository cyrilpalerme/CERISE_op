{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import h5py\n",
    "import netCDF4\n",
    "import scipy\n",
    "import pyproj\n",
    "import pyresample\n",
    "import datetime\n",
    "import torch\n",
    "import torchsummary\n",
    "import torch_geometric\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Check if a GPU is available\n",
    "print(\"Using device: \"  + str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"v6\"\n",
    "AMSR2_frequency = \"18.7\"\n",
    "#\n",
    "function_path = \"/lustre/storeA/users/cyrilp/CERISE/Models_static/\" + experiment_name + \"/\"\n",
    "sys.path.insert(0, function_path)\n",
    "from Data_generator_GNN_prediction import *\n",
    "from GNN_GAT import *\n",
    "#\n",
    "paths = {}\n",
    "paths[\"training\"] = \"/lustre/storeB/project/nwp/H2O/wp3/Deep_learning_predictions/Training_data_GNN/\" + AMSR2_frequency.split('.')[0] + \"GHz_static/\"\n",
    "paths[\"normalization\"] = \"/lustre/storeB/project/nwp/H2O/wp3/Deep_learning_predictions/Normalization/\"\n",
    "paths[\"model\"] = \"/lustre/storeB/project/nwp/H2O/wp3/Deep_learning_predictions/GNN/Models_static/\" + experiment_name + \"/\"\n",
    "paths[\"surfex_grid\"] = \"/lustre/storeB/users/josteinbl/sfx_data/LDAS_NOR/climate/\"\n",
    "paths[\"output\"] = \"/lustre/storeB/project/nwp/H2O/wp3/Deep_learning_predictions/GNN/Models_static/\" + experiment_name + \"/Predictions_\" + AMSR2_frequency.split('.')[0] + \"GHz/\"\n",
    "#\n",
    "filename_normalization = paths[\"normalization\"] + \"Stats_normalization_20200901_20220531.h5\"\n",
    "#\n",
    "for var in paths:\n",
    "    if os.path.isdir(paths[var]) == False:\n",
    "        os.system(\"mkdir -p \" + paths[var])\n",
    "#\n",
    "AMSR2_all_frequencies = [\"6.9\", \"7.3\", \"10.7\", \"18.7\", \"23.8\", \"36.5\"]\n",
    "AMSR2_all_footprint_radius = np.array([35 + 62, 35 + 62, 24 + 42, 14 + 22, 11 + 19, 7 + 12]) * 0.25 * 1000  # 0.5 * mean diameter (0.5 * (major + minor)), *1000 => km to meters\n",
    "AMSR2_footprint_radius = AMSR2_all_footprint_radius[AMSR2_all_frequencies.index(AMSR2_frequency)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_min = \"20220901\"\n",
    "date_max = \"20230531\"\n",
    "subsampling = \"1\"\n",
    "#\n",
    "def he_normal_init(weight):\n",
    "    torch.nn.init.kaiming_normal_(weight, mode = \"fan_out\", nonlinearity = \"relu\")\n",
    "weight_initializer = he_normal_init\n",
    "#\n",
    "activation = torch.nn.ReLU()\n",
    "shuffle = True\n",
    "conv_filers = [32, 64, 32]\n",
    "batch_size = 512\n",
    "batch_normalization = True\n",
    "attention_heads = 4\n",
    "#\n",
    "predictors = {}\n",
    "predictors[\"constants\"] = [\"ZS\", \"PATCHP1\", \"PATCHP2\", \"FRAC_LAND_AND_SEA_WATER\", \"Distance_to_footprint_center\"]\n",
    "predictors[\"atmosphere\"] = [\"lwe_thickness_of_atmosphere_mass_content_of_water_vapor\"]\n",
    "#predictors[\"ISBA\"] = [\"Q2M_ISBA\", \"DSN_T_ISBA\", \"LAI_ga\", \"TS_ISBA\", \"PSN_ISBA\"]\n",
    "predictors[\"ISBA\"] = [\"LAI_ga\", \"DSN_T_ISBA\", \"WSN_T_ISBA\"]\n",
    "predictors[\"TG\"] = [1, 2]\n",
    "predictors[\"WG\"] = [1, 2]\n",
    "predictors[\"WGI\"] = [1, 2]\n",
    "#predictors[\"WSN_VEG\"] = [1, 6, 12]\n",
    "predictors[\"RSN_VEG\"] = [1, 6, 12]\n",
    "predictors[\"HSN_VEG\"] = [1, 6, 12]\n",
    "predictors[\"SNOWTEMP\"] = [1, 6, 12]\n",
    "predictors[\"SNOWLIQ\"] = [1, 6, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list_dates(date_min, date_max):\n",
    "    current_date = datetime.datetime.strptime(date_min, \"%Y%m%d\")\n",
    "    end_date = datetime.datetime.strptime(date_max, \"%Y%m%d\")\n",
    "    list_dates = []\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime(\"%Y%m%d\")\n",
    "        list_dates.append(date_str)\n",
    "        current_date = current_date + datetime.timedelta(days = 1)\n",
    "    return(list_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Surfex coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_surfex_coordinates():\n",
    "    def __init__(self, paths):\n",
    "        self.inputgrid = paths[\"surfex_grid\"] + \"PGD.nc\"\n",
    "    #\n",
    "    def sfx2areadef(self, lat0, lon0, latori, lonori, xx, yy):\n",
    "        proj2 = \"+proj=lcc +lat_1=%.2f +lat_2=%.2f +lat_0=%.2f +lon_0=%.2f +units=m +ellps=WGS84 +no_defs\" % (lat0,lat0,lat0,lon0)\n",
    "        p2 = pyproj.Proj(proj2, preserve_units = False)\n",
    "        origo = p2(lonori.data, latori.data)\n",
    "        extent = origo + (origo[0] + xx[-1,-1], origo[1] + yy[-1,-1])\n",
    "        area_def = pyresample.geometry.AreaDefinition(\"id2\", \"hei2\", \"lcc\", proj2, xx.shape[1], yy.shape[0], extent)\n",
    "        return(area_def)\n",
    "    #\n",
    "    def getSFXgrid(self):\n",
    "        with netCDF4.Dataset(self.inputgrid, \"r\") as nc:\n",
    "            areadef = self.sfx2areadef(lat0 = nc[\"LAT0\"][0], lon0 = nc[\"LON0\"][0], latori = nc[\"LATORI\"][0], lonori = nc[\"LONORI\"][0], xx = nc[\"XX\"][:], yy = nc[\"YY\"][:])\n",
    "        return(areadef)\n",
    "    #\n",
    "    def __call__(self):\n",
    "        areadef = self.getSFXgrid()\n",
    "        lon, lat = areadef.get_lonlats()\n",
    "        Surfex_coord = {}\n",
    "        Surfex_coord[\"proj4_string\"] = areadef.proj4_string\n",
    "        Surfex_coord[\"crs\"] = areadef.crs\n",
    "        Surfex_coord[\"lon\"] = lon\n",
    "        Surfex_coord[\"lat\"] = lat\n",
    "        transform_to_surfex = pyproj.Transformer.from_crs(pyproj.CRS.from_proj4(\"+proj=latlon\"), Surfex_coord[\"crs\"], always_xy = True)\n",
    "        Surfex_coord[\"xx\"], Surfex_coord[\"yy\"] = transform_to_surfex.transform(Surfex_coord[\"lon\"], Surfex_coord[\"lat\"])\n",
    "        Surfex_coord[\"x\"] = Surfex_coord[\"xx\"][0,:]\n",
    "        Surfex_coord[\"y\"] = Surfex_coord[\"yy\"][:,0]\n",
    "        return(Surfex_coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_model_parameters():\n",
    "    def __init__(self, AMSR2_frequency, filename_normalization, predictors, activation, weight_initializer, conv_filters, batch_normalization, attention_heads):\n",
    "        self.AMSR2_frequency = AMSR2_frequency\n",
    "        self.filename_normalization = filename_normalization\n",
    "        self.predictors = predictors\n",
    "        self.activation = activation\n",
    "        self.weight_initializer = weight_initializer\n",
    "        self.conv_filters = conv_filters\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.attention_heads = attention_heads\n",
    "    #\n",
    "    def load_normalization_stats(self):\n",
    "        normalization_stats = {}\n",
    "        with h5py.File(self.filename_normalization) as hdf:\n",
    "            for var in hdf:\n",
    "                normalization_stats[var] = hdf[var][()]\n",
    "        return(normalization_stats)\n",
    "    #\n",
    "    def make_list_predictors(self):\n",
    "        list_predictors = self.predictors[\"constants\"] + self.predictors[\"atmosphere\"] + self.predictors[\"ISBA\"]\n",
    "        for pred in self.predictors:\n",
    "            if (pred != \"constants\") and (pred != \"atmosphere\") and (pred != \"ISBA\"):\n",
    "                for lay in self.predictors[pred]:\n",
    "                    var_name = pred + str(lay) + \"_ga\"\n",
    "                    list_predictors = list_predictors + [var_name]\n",
    "        return(list_predictors)\n",
    "    #\n",
    "    def make_list_targets(self):\n",
    "        list_targets = [\"AMSR2_BT\" + self.AMSR2_frequency + \"H\", \"AMSR2_BT\" + self.AMSR2_frequency + \"V\"]\n",
    "        return(list_targets)\n",
    "    #\n",
    "    def make_model_parameters(self, list_predictors, list_targets):\n",
    "        model_params = {\"list_predictors\": list_predictors,\n",
    "                        \"list_targets\": list_targets,\n",
    "                        \"activation\": self.activation,\n",
    "                        \"weight_initializer\": self.weight_initializer,\n",
    "                        \"conv_filters\": self.conv_filters,\n",
    "                        \"batch_normalization\": self.batch_normalization,\n",
    "                        \"heads\": self.attention_heads,\n",
    "                        }\n",
    "        return(model_params)\n",
    "    #\n",
    "    def __call__(self):\n",
    "        normalization_stats = self.load_normalization_stats()\n",
    "        list_predictors = self.make_list_predictors()\n",
    "        list_targets = self.make_list_targets()\n",
    "        model_params = self.make_model_parameters(list_predictors, list_targets)\n",
    "        return(normalization_stats, model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_loader():\n",
    "    def __init__(self, AMSR2_frequency, AMSR2_footprint_radius, list_predictors, normalization_stats, date_task, paths):\n",
    "        self.AMSR2_frequency = AMSR2_frequency\n",
    "        self.AMSR2_footprint_radius = AMSR2_footprint_radius \n",
    "        self.list_predictors = list_predictors\n",
    "        self.normalization_stats = normalization_stats\n",
    "        self.date_task = date_task\n",
    "        self.paths = paths\n",
    "        self.filename_data = self.paths[\"training\"] + self.date_task[0:4] + \"/\" + self.date_task[4:6] + \"/\" + \"Graphs_\" + self.date_task + \".h5\"\n",
    "    #\n",
    "    def Number_of_samples_and_footprint_coordinates(self):\n",
    "        Graphs_coord = {}\n",
    "        Targets = {}\n",
    "        #\n",
    "        with h5py.File(self.filename_data, \"r\") as hdf:\n",
    "            Number_of_graphs = len(hdf[\"AMSR2_xx\"][()])\n",
    "            #\n",
    "            Graphs_coord[\"xx\"] = np.full(Number_of_graphs, np.nan)\n",
    "            Graphs_coord[\"yy\"] = np.full(Number_of_graphs, np.nan)\n",
    "            Targets[\"AMSR2_BT\" + self.AMSR2_frequency + \"H\"] = np.full(Number_of_graphs, np.nan)\n",
    "            Targets[\"AMSR2_BT\" + self.AMSR2_frequency + \"V\"] = np.full(Number_of_graphs, np.nan)\n",
    "            #\n",
    "            Graphs_coord[\"xx\"] = hdf[\"AMSR2_xx\"][()]\n",
    "            Graphs_coord[\"yy\"] = hdf[\"AMSR2_yy\"][()]\n",
    "            Targets[\"AMSR2_BT\" + self.AMSR2_frequency + \"H\"] = hdf[\"AMSR2_BT\" + self.AMSR2_frequency + \"H\"][()]\n",
    "            Targets[\"AMSR2_BT\" + self.AMSR2_frequency + \"V\"] = hdf[\"AMSR2_BT\" + self.AMSR2_frequency + \"V\"][()]\n",
    "            #\n",
    "            return(Number_of_graphs, Graphs_coord, Targets)\n",
    "    #\n",
    "    def make_data_generator_parameters(self, filename_data):\n",
    "        data_generator_params = {\"filename_data\": filename_data,\n",
    "                                 \"footprint_radius\": self.AMSR2_footprint_radius,\n",
    "                                 \"list_predictors\": self.list_predictors,\n",
    "                                 \"normalization_stats\": self.normalization_stats}\n",
    "        return(data_generator_params)\n",
    "    #\n",
    "    def create_data_loader(self, Number_of_graphs, data_generator_params):\n",
    "        dataset = Data_generator_GNN_prediction(**data_generator_params)\n",
    "        return(dataset)\n",
    "    #\n",
    "    def __call__(self):\n",
    "        Number_of_graphs, Graphs_coord, Targets = self.Number_of_samples_and_footprint_coordinates()\n",
    "        params_valid = self.make_data_generator_parameters(self.filename_data)\n",
    "        valid_loader = self.create_data_loader(Number_of_graphs, params_valid)\n",
    "        return(Number_of_graphs, Graphs_coord, Targets, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_predictions():\n",
    "    def __init__(self, list_targets, model, valid_loader, paths, normalization_stats, device):\n",
    "        self.list_targets = list_targets\n",
    "        self.model = model\n",
    "        self.valid_loader = valid_loader\n",
    "        self.paths = paths\n",
    "        self.normalization_stats = normalization_stats\n",
    "        self.device = device\n",
    "    #\n",
    "    def unnormalize(self, unnormalized_predictions):\n",
    "        normalized_predictions = np.full(np.shape(unnormalized_predictions), np.nan)\n",
    "        for vi, var in enumerate(self.list_targets):\n",
    "            normalized_predictions[:, vi] = unnormalized_predictions[:, vi] * (self.normalization_stats[var + \"_max\"] - self.normalization_stats[var + \"_min\"]) + self.normalization_stats[var + \"_min\"]\n",
    "        return(normalized_predictions)\n",
    "    #\n",
    "    def predictions(self):\n",
    "        self.model.eval()\n",
    "        print(len(self.valid_loader))\n",
    "        with torch.no_grad(), torch.amp.autocast(device_type = \"cuda\"):\n",
    "            for batch_valid in self.valid_loader:\n",
    "                data_batch = batch_valid.to(self.device)\n",
    "                x, y, a, batch = data_batch.x, data_batch.y, data_batch.edge_index, data_batch.batch\n",
    "                print(\"x,y,a\", np.shape(x))\n",
    "                unnormalized_predictions = self.model(x, a, batch)        \n",
    "                print(\"unnormalized predictions\")\n",
    "        unnormalized_predictions = unnormalized_predictions.cpu().numpy()            \n",
    "        print(\"to numpy\")         \n",
    "        return(unnormalized_predictions)\n",
    "    #\n",
    "    def __call__(self):\n",
    "        unnormalized_predictions = self.predictions()\n",
    "        normalized_predictions = self.unnormalize(unnormalized_predictions)\n",
    "        return(normalized_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Gridding predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gridding_predictions():\n",
    "    def __init__(self, date_task, AMSR2_footprint_radius, Surfex_coord, list_targets, Targets, Graphs_coord, predictions, paths):\n",
    "        self.date_task = date_task\n",
    "        self.AMSR2_footprint_radius = AMSR2_footprint_radius \n",
    "        self.Surfex_coord = Surfex_coord\n",
    "        self.list_targets = list_targets\n",
    "        self.idx_nan = np.logical_or(np.isnan(Graphs_coord[\"xx\"]) == True, np.isnan(Graphs_coord[\"yy\"]) == True)\n",
    "        self.idx_nan_extend = np.repeat(np.expand_dims(self.idx_nan, axis = 1), len(self.list_targets), axis = 1)\n",
    "        self.Targets = Targets\n",
    "        for var in self.Targets:\n",
    "            self.Targets[var] = self.Targets[var][self.idx_nan == False]\n",
    "        self.Graphs_xx = Graphs_coord[\"xx\"][self.idx_nan == False]\n",
    "        self.Graphs_yy = Graphs_coord[\"yy\"][self.idx_nan == False]\n",
    "        self.predictions = predictions[self.idx_nan_extend == False]\n",
    "        self.paths = paths\n",
    "    #\n",
    "    def nearest_neighbor_indexes(self):\n",
    "        pred_xx = np.expand_dims(self.Graphs_xx, axis = 1)\n",
    "        pred_yy = np.expand_dims(self.Graphs_yy, axis = 1)\n",
    "        Surfex_xx = np.expand_dims(np.ndarray.flatten(self.Surfex_coord[\"xx\"]), axis = 1)\n",
    "        Surfex_yy = np.expand_dims(np.ndarray.flatten(self.Surfex_coord[\"yy\"]), axis = 1)\n",
    "        #\n",
    "        coord_input = np.concatenate((pred_xx, pred_yy), axis = 1)\n",
    "        coord_output = np.concatenate((Surfex_xx, Surfex_yy), axis = 1)\n",
    "        #\n",
    "        tree = scipy.spatial.KDTree(coord_input)\n",
    "        dist, idx = tree.query(coord_output)\n",
    "        return(dist, idx)\n",
    "    #\n",
    "    def project_predictions_onto_Surfex_domain(self):\n",
    "        dist, idx = self.nearest_neighbor_indexes()\n",
    "        Gridded_distance = np.reshape(dist, (len(self.Surfex_coord[\"y\"]), len(self.Surfex_coord[\"x\"])), order = \"C\")\n",
    "        #\n",
    "        Gridded_targets = {}\n",
    "        for vi, var in enumerate(self.list_targets):\n",
    "            Target_interp = np.ndarray.flatten(self.Targets[var])[idx]\n",
    "            Gridded_targets[var] = np.reshape(Target_interp, (len(self.Surfex_coord[\"y\"]), len(self.Surfex_coord[\"x\"])), order = \"C\")\n",
    "            Gridded_targets[var][Gridded_distance > self.AMSR2_footprint_radius] = np.nan\n",
    "        #\n",
    "        Gridded_predictions = {}\n",
    "        for vi, var in enumerate(self.list_targets):\n",
    "            Pred_interp = np.ndarray.flatten(predictions[:, vi])[idx]\n",
    "            Gridded_predictions[var] = np.reshape(Pred_interp, (len(self.Surfex_coord[\"y\"]), len(self.Surfex_coord[\"x\"])), order = \"C\")\n",
    "            Gridded_predictions[var][Gridded_distance > self.AMSR2_footprint_radius] = np.nan\n",
    "        #\n",
    "        #Gridded_distance[Gridded_distance > self.AMSR2_footprint_radius] = np.nan\n",
    "        #\n",
    "        return(Gridded_predictions, Gridded_targets, Gridded_distance)\n",
    "    #\n",
    "    def write_netCDF(self, Gridded_predictions, Gridded_targets, Gridded_distance):\n",
    "        path_output = self.paths[\"output\"] + self.date_task[0:4] + \"/\" + self.date_task[4:6] + \"/\"\n",
    "        if os.path.exists(path_output) == False:\n",
    "            os.system(\"mkdir -p \" + path_output)\n",
    "        output_filename = path_output + \"Predictions_\" + self.date_task + \".nc\"\n",
    "        if os.path.isfile(output_filename):\n",
    "            os.system(\"rm \" + output_filename)\n",
    "        #\n",
    "        with netCDF4.Dataset(str(output_filename), \"w\", format = \"NETCDF4\") as output_netcdf:\n",
    "            x = output_netcdf.createDimension(\"x\", len(self.Surfex_coord[\"x\"]))\n",
    "            y = output_netcdf.createDimension(\"y\", len(self.Surfex_coord[\"y\"]))\n",
    "            #\n",
    "            Outputs = vars()\n",
    "            for var in [\"x\", \"y\"]:\n",
    "                Outputs[var] = output_netcdf.createVariable(var, \"d\", (var))\n",
    "                Outputs[var].units = \"meters\" \n",
    "                Outputs[var].standard_name = \"projection_\" + var + \"_coordinates\"\n",
    "                Outputs[var] = np.copy(self.Surfex_coord[var])\n",
    "            #\n",
    "            for var in [\"lat\", \"lon\"]:\n",
    "                Outputs[var] = output_netcdf.createVariable(var, \"d\", (\"y\", \"x\"))\n",
    "                if var == \"lat\":\n",
    "                    Outputs[var].standard_name = \"latitude\"\n",
    "                    Outputs[var].unit = \"degrees_north\"\n",
    "                else:\n",
    "                    Outputs[var].standard_name = \"longitude\"\n",
    "                    Outputs[var].units = \"degrees_east\"\n",
    "                Outputs[var][:,:] = np.copy(self.Surfex_coord[var])\n",
    "            #\n",
    "            for var in Gridded_targets:\n",
    "                Outputs[\"Target_\" + var] = output_netcdf.createVariable(\"Target_\" + var, \"d\", (\"y\", \"x\"))\n",
    "                Outputs[\"Target_\" + var].units = \"Kelvins\"\n",
    "                Outputs[\"Target_\" + var].standard_name = \"Brightness temperature\"\n",
    "                Outputs[\"Target_\" + var][:,:] = np.copy(Gridded_targets[var])\n",
    "            #\n",
    "            for var in Gridded_predictions:\n",
    "                Outputs[\"Prediction_\" + var] = output_netcdf.createVariable(\"Prediction_\" + var, \"d\", (\"y\", \"x\"))\n",
    "                Outputs[\"Prediction_\" + var].units = \"Kelvins\"\n",
    "                Outputs[\"Prediction_\" + var].standard_name = \"Brightness temperature\"\n",
    "                Outputs[\"Prediction_\" + var][:,:] = np.copy(Gridded_predictions[var])\n",
    "            #\n",
    "            Outputs[\"Distance_to_footprint_center\"] = output_netcdf.createVariable(\"Distance_to_footprint_center\", \"d\", (\"y\", \"x\"))\n",
    "            Outputs[\"Distance_to_footprint_center\"].units = \"meters\"\n",
    "            Outputs[\"Distance_to_footprint_center\"].standard_name = \"Distance_to_footprint_center\"\n",
    "            Outputs[\"Distance_to_footprint_center\"][:,:] = np.copy(Gridded_distance)\n",
    "    #\n",
    "    def __call__(self):\n",
    "        Gridded_predictions, Gridded_targets, Gridded_distance = self.project_predictions_onto_Surfex_domain()\n",
    "        self.write_netCDF(Gridded_predictions, Gridded_targets, Gridded_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/modules/rhel8/conda/install/envs/production-08-2024/lib/python3.9/site-packages/pyproj/crs/crs.py:1282: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n",
      "  proj = self._crs.to_proj4(version=version)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m Surfex_coord \u001b[38;5;241m=\u001b[39m get_surfex_coordinates(paths)()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGNN_model_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m AMSR2_frequency\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGHz.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, weights_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      7\u001b[0m normalization_stats, model_params \u001b[38;5;241m=\u001b[39m make_model_parameters(AMSR2_frequency \u001b[38;5;241m=\u001b[39m AMSR2_frequency, \n\u001b[1;32m      8\u001b[0m                                                           filename_normalization \u001b[38;5;241m=\u001b[39m filename_normalization, \n\u001b[1;32m      9\u001b[0m                                                           predictors \u001b[38;5;241m=\u001b[39m predictors, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m                                                           batch_normalization \u001b[38;5;241m=\u001b[39m batch_normalization,\n\u001b[1;32m     14\u001b[0m                                                           attention_heads \u001b[38;5;241m=\u001b[39m attention_heads)()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py:1360\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1358\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m-> 1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1368\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1851\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py:1812\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1810\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1811\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1812\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1814\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py:1784\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1779\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1784\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1785\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1786\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1787\u001b[0m )\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1790\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py:601\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 601\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py:539\u001b[0m, in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    537\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_privateuse1_backend_name()\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[0;32m--> 539\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py:508\u001b[0m, in \u001b[0;36m_validate_device\u001b[0;34m(location, backend_name)\u001b[0m\n\u001b[1;32m    506\u001b[0m     device_index \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_available\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 508\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.is_available() is False. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m     )\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_count\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    516\u001b[0m     device_count \u001b[38;5;241m=\u001b[39m device_module\u001b[38;5;241m.\u001b[39mdevice_count()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "tt0 = time.time()\n",
    "#\n",
    "Surfex_coord = get_surfex_coordinates(paths)()\n",
    "#\n",
    "checkpoint = torch.load(paths[\"model\"] + \"GNN_model_\" + AMSR2_frequency.split('.')[0] + \"GHz.pth\", weights_only = False) \n",
    "#\n",
    "normalization_stats, model_params = make_model_parameters(AMSR2_frequency = AMSR2_frequency, \n",
    "                                                          filename_normalization = filename_normalization, \n",
    "                                                          predictors = predictors, \n",
    "                                                          activation = activation, \n",
    "                                                          weight_initializer = weight_initializer, \n",
    "                                                          conv_filters = conv_filers, \n",
    "                                                          batch_normalization = batch_normalization,\n",
    "                                                          attention_heads = attention_heads)()\n",
    "#\n",
    "list_dates = make_list_dates(date_min, date_max)\n",
    "for date_task in list_dates:\n",
    "    try:\n",
    "        Number_of_graphs, Graphs_coord, Targets, valid_loader = make_loader(AMSR2_frequency = AMSR2_frequency,\n",
    "                                                                            AMSR2_footprint_radius = AMSR2_footprint_radius, \n",
    "                                                                            list_predictors = model_params[\"list_predictors\"], \n",
    "                                                                            normalization_stats = normalization_stats,\n",
    "                                                                            date_task = date_task,\n",
    "                                                                            paths = paths)()\n",
    "        #\n",
    "        GNN_model = GNN_GAT(**model_params).to(device)\n",
    "        GNN_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        #\n",
    "        print(\"Pred starts\")\n",
    "        t0 = time.time()\n",
    "        predictions = make_predictions(list_targets = model_params[\"list_targets\"], \n",
    "                                    model = GNN_model, \n",
    "                                    valid_loader = valid_loader, \n",
    "                                    paths = paths, \n",
    "                                    normalization_stats = normalization_stats, \n",
    "                                    device = device)()\n",
    "        #\n",
    "        print(\"Pred OK\")\n",
    "        t1 = time.time()\n",
    "        print(date_task, t1 - t0)\n",
    "        #\n",
    "        gridding_predictions(date_task = date_task, \n",
    "                            AMSR2_footprint_radius = AMSR2_footprint_radius,\n",
    "                            Surfex_coord = Surfex_coord, \n",
    "                            list_targets = model_params[\"list_targets\"], \n",
    "                            Targets = Targets, \n",
    "                            Graphs_coord = Graphs_coord, \n",
    "                            predictions = predictions, \n",
    "                            paths = paths)()\n",
    "    except:\n",
    "       pass\n",
    "#\n",
    "ttf = time.time()\n",
    "print(\"Total computing time: \", ttf - tt0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "production-08-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
