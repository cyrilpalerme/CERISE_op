{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import zarr\n",
    "import pandas\n",
    "import random\n",
    "import datetime\n",
    "import logging\n",
    "import torch\n",
    "import torchsummary\n",
    "import torch_geometric\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Check if a GPU is available\n",
    "print(\"Using device: \"  + str(device))\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"v6\"\n",
    "AMSR2_frequency = \"18.7\"\n",
    "#\n",
    "function_path = \"/lustre/storeB/users/cyrilp/CERISE/Scripts/GNN/Model_static/\" + experiment_name + \"/\"\n",
    "sys.path.insert(0, function_path)\n",
    "from Data_generator_GNN import *\n",
    "from GNN_GAT import *\n",
    "#\n",
    "paths = {}\n",
    "paths[\"training\"] = \"/lustre/storeB/project/nwp/H2O/wp3/Deep_learning_predictions/Training_data_GNN/\" + AMSR2_frequency.split('.')[0] + \"GHz_static/\"\n",
    "paths[\"normalization\"] = \"/lustre/storeB/project/nwp/H2O/wp3/Deep_learning_predictions/Normalization/\"\n",
    "paths[\"output\"] = \"/lustre/storeB/project/nwp/H2O/wp3/Deep_learning_predictions/GNN/Models_static/\" + experiment_name + \"/\"\n",
    "#\n",
    "filename_normalization = paths[\"normalization\"] + \"Stats_normalization_20200901_20220531.h5\"\n",
    "#\n",
    "for var in paths:\n",
    "    if os.path.isdir(paths[var]) == False:\n",
    "        os.system(\"mkdir -p \" + paths[var])\n",
    "#\n",
    "AMSR2_all_frequencies = [\"6.9\", \"7.3\", \"10.7\", \"18.7\", \"23.8\", \"36.5\"]\n",
    "AMSR2_all_footprint_radius = np.array([35 + 62, 35 + 62, 24 + 42, 14 + 22, 11 + 19, 7 + 12]) * 0.25 * 1000  # 0.5 * mean diameter (0.5 * (major + minor)), *1000 => km to meters\n",
    "AMSR2_footprint_radius = AMSR2_all_footprint_radius[AMSR2_all_frequencies.index(AMSR2_frequency)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_min_train = \"20200901\"\n",
    "date_max_train = \"20220531\"\n",
    "date_min_valid = \"20220901\"\n",
    "date_max_valid = \"20230531\"\n",
    "subsampling = \"1\"\n",
    "#\n",
    "def he_normal_init(weight):\n",
    "    torch.nn.init.kaiming_normal_(weight, mode = \"fan_out\", nonlinearity = \"relu\")\n",
    "weight_initializer = he_normal_init\n",
    "weight_initializer_str = \"he_normal\"\n",
    "#\n",
    "activation = torch.nn.ReLU()\n",
    "shuffle = False\n",
    "conv_filers = [32, 64, 32]\n",
    "batch_size = 512\n",
    "batch_normalization = True\n",
    "attention_heads = 4\n",
    "#\n",
    "predictors = {}\n",
    "predictors[\"constants\"] = [\"ZS\", \"PATCHP1\", \"PATCHP2\", \"FRAC_LAND_AND_SEA_WATER\", \"Distance_to_footprint_center\"]\n",
    "predictors[\"atmosphere\"] = [\"lwe_thickness_of_atmosphere_mass_content_of_water_vapor\"]\n",
    "#predictors[\"ISBA\"] = [\"Q2M_ISBA\", \"DSN_T_ISBA\", \"LAI_ga\", \"TS_ISBA\", \"PSN_ISBA\"]\n",
    "predictors[\"ISBA\"] = [\"LAI_ga\", \"DSN_T_ISBA\", \"WSN_T_ISBA\"]\n",
    "predictors[\"TG\"] = [1, 2]\n",
    "predictors[\"WG\"] = [1, 2]\n",
    "predictors[\"WGI\"] = [1, 2]\n",
    "#predictors[\"WSN_VEG\"] = [1, 6, 12]\n",
    "predictors[\"RSN_VEG\"] = [1, 6, 12]\n",
    "predictors[\"HSN_VEG\"] = [1, 6, 12]\n",
    "predictors[\"SNOWTEMP\"] = [1, 6, 12]\n",
    "predictors[\"SNOWLIQ\"] = [1, 6, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_params = {\"loss_function\": torch.nn.MSELoss(),    # Loss function\n",
    "                  \"initial_learning_rate\": 0.0025,        # Initial learning rate\n",
    "                  \"step_size\": 5,                         # Define how many epochs are computed before the learning rate is decreased.\n",
    "                  \"gamma\": 0.5,                           # Define the factor used to decrease the learning rate. If 0.25, the learning rate is divided by 4.\n",
    "                  \"n_epochs\": 30,                         # Number of epochs used for training the model\n",
    "                  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_details(model, paths, weight_initializer_str, shuffle, batch_normalization, attention_heads, batch_size, compile_params, predictors, AMSR2_frequency, date_min_train, date_max_train, date_min_valid, date_max_valid, subsampling):\n",
    "    filename = paths[\"output\"] + \"Model_details_\" + AMSR2_frequency + \"GHz_\" + datetime.datetime.now().strftime(\"%Y%m%d\") + \".txt\"\n",
    "    if os.path.isfile(filename):\n",
    "        os.system(\"rm \" + filename)\n",
    "    #\n",
    "    list_predictors = predictors[\"constants\"] + predictors[\"atmosphere\"] + predictors[\"ISBA\"]\n",
    "    for pred in predictors:\n",
    "        if (pred != \"constants\") and (pred != \"atmosphere\") and (pred != \"ISBA\"):\n",
    "            for lay in predictors[pred]:\n",
    "                var_name = pred + str(lay) + \"_ga\"\n",
    "                list_predictors = list_predictors + [var_name]\n",
    "    predictors_str = \"\"\n",
    "    for var in list_predictors:\n",
    "        predictors_str = predictors_str + var + \", \"\n",
    "    #\n",
    "    params_str = \"AMSR2_frequency: \" + AMSR2_frequency + \"\\n\" + \\\n",
    "                 \"weight_initializer: \" + weight_initializer_str + \"\\n\" + \\\n",
    "                 \"shuffle: \" + str(shuffle) + \"\\n\" + \\\n",
    "                 \"batch_normalization: \" + str(batch_normalization) + \"\\n\" + \\\n",
    "                 \"attention_heads: \" + str(attention_heads) + \"\\n\" + \\\n",
    "                 \"batch_size: \" + str(batch_size) + \"\\n\" + \\\n",
    "                 \"date_min_train: \" + date_min_train + \"\\n\" + \\\n",
    "                 \"date_max_train: \" + date_max_train + \"\\n\" + \\\n",
    "                 \"date_min_valid: \" + date_min_valid + \"\\n\" + \\\n",
    "                 \"date_max_valid: \" + date_max_valid + \"\\n\" + \\\n",
    "                 \"subsampling: \" + subsampling + \"\\n\"\n",
    "    #\n",
    "    for var in compile_params:\n",
    "        params_str = params_str + var + \" \" + str(compile_params[var]) + \"\\n\"\n",
    "    #\n",
    "    with open(filename, \"w\") as log_file:\n",
    "        log_file.write(params_str)\n",
    "        log_file.write(\"list_predictors: \" + predictors_str + \"\\n\")\n",
    "    #     \n",
    "    logging.basicConfig(filename = filename, level = logging.INFO)  # Save model details in a text file \n",
    "    logging.info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_parameters():\n",
    "    def __init__(self, paths, filename_normalization, AMSR2_frequency, AMSR2_footprint_radius, predictors, activation, weight_initializer, conv_filters, batch_size, batch_normalization, attention_heads, shuffle, date_min_train, date_max_train, date_min_valid, date_max_valid, subsampling):\n",
    "        self.paths = paths\n",
    "        self.filename_normalization = filename_normalization\n",
    "        self.AMSR2_frequency = AMSR2_frequency\n",
    "        self.AMSR2_footprint_radius = AMSR2_footprint_radius \n",
    "        self.predictors = predictors\n",
    "        self.activation = activation\n",
    "        self.weight_initializer = weight_initializer\n",
    "        self.conv_filters = conv_filters\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.attention_heads = attention_heads\n",
    "        self.shuffle = shuffle\n",
    "        self.date_min_train = date_min_train\n",
    "        self.date_max_train = date_max_train\n",
    "        self.date_min_valid = date_min_valid\n",
    "        self.date_max_valid = date_max_valid\n",
    "        self.subsampling = subsampling\n",
    "        self.filename_train = self.paths[\"training\"] + \"Graphs_\" + self.date_min_train + \"_\" + self.date_max_train + \"_subsampling_\" + self.subsampling + \".zarr\"\n",
    "        self.filename_valid = self.paths[\"training\"] + \"Graphs_\" + self.date_min_valid + \"_\" + self.date_max_valid + \"_subsampling_\" + self.subsampling + \".zarr\"\n",
    "    #\n",
    "    def make_list_predictors(self):\n",
    "        list_predictors = predictors[\"constants\"] + predictors[\"atmosphere\"] + predictors[\"ISBA\"]\n",
    "        for pred in predictors:\n",
    "            if (pred != \"constants\") and (pred != \"atmosphere\") and (pred != \"ISBA\"):\n",
    "                for lay in predictors[pred]:\n",
    "                    var_name = pred + str(lay) + \"_ga\"\n",
    "                    list_predictors = list_predictors + [var_name]\n",
    "        return(list_predictors)\n",
    "    #\n",
    "    def make_list_targets(self):\n",
    "        list_targets = [\"AMSR2_BT\" + self.AMSR2_frequency + \"H\", \"AMSR2_BT\" + self.AMSR2_frequency + \"V\"]\n",
    "        return(list_targets)\n",
    "    #\n",
    "    def load_normalization_stats(self):\n",
    "        normalization_stats = {}\n",
    "        with h5py.File(self.filename_normalization) as hdf:\n",
    "            for var in hdf:\n",
    "                normalization_stats[var] = hdf[var][()]\n",
    "        return(normalization_stats)\n",
    "    #\n",
    "    def make_model_parameters(self, list_predictors, list_targets):\n",
    "        model_params = {\"list_predictors\": list_predictors,\n",
    "                        \"list_targets\": list_targets,\n",
    "                        \"activation\": self.activation,\n",
    "                        \"weight_initializer\": self.weight_initializer,\n",
    "                        \"conv_filters\": self.conv_filters,\n",
    "                        \"batch_normalization\": self.batch_normalization,\n",
    "                        \"heads\": self.attention_heads,\n",
    "                        }\n",
    "        return(model_params)\n",
    "    #\n",
    "    def make_data_generator_parameters(self, filename_data, list_predictors, list_targets):\n",
    "        data_generator_params = {\"filename_data\": filename_data,\n",
    "                                 \"footprint_radius\": self.AMSR2_footprint_radius,\n",
    "                                 \"list_predictors\": list_predictors,\n",
    "                                 \"list_targets\": list_targets,\n",
    "                                 \"normalization_stats\": self.load_normalization_stats(),\n",
    "                                 \"batch_size\": self.batch_size,\n",
    "                                 \"paths\": self.paths}\n",
    "        return(data_generator_params)\n",
    "    #\n",
    "    def create_data_loader(self, data_generator_params):\n",
    "        dataset = Data_generator_GNN(**data_generator_params)\n",
    "        return(dataset)\n",
    "    #\n",
    "    def __call__(self):\n",
    "        list_predictors = self.make_list_predictors()\n",
    "        list_targets = self.make_list_targets()\n",
    "        model_params = self.make_model_parameters(list_predictors, list_targets)\n",
    "        params_train = self.make_data_generator_parameters(self.filename_train, list_predictors, list_targets)\n",
    "        params_valid = self.make_data_generator_parameters(self.filename_valid, list_predictors, list_targets)\n",
    "        train_loader = self.create_data_loader(params_train)\n",
    "        valid_loader = self.create_data_loader(params_valid)\n",
    "        return(model_params, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_model():\n",
    "    def __init__(self, AMSR2_frequency, model, train_loader, valid_loader, compile_params, paths, filename_normalization, device, initial_time):\n",
    "        self.AMSR2_frequency = AMSR2_frequency\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.compile_params = compile_params\n",
    "        self.paths = paths\n",
    "        self.hdf_normalization = h5py.File(filename_normalization)\n",
    "        self.device = device\n",
    "        self.initial_time = initial_time \n",
    "        self.filename_training_stats = paths[\"output\"] + \"Training_statistics_\" + self.AMSR2_frequency.split('.')[0] + \"GHz_\" + datetime.datetime.now().strftime(\"%Y%m%d\") + \".txt\"\n",
    "        self.filename_model = paths[\"output\"] + \"GNN_model_\" + self.AMSR2_frequency.split('.')[0] + \"GHz.pth\"\n",
    "        self.loss_function = compile_params[\"loss_function\"]\n",
    "        self.optimizer =  torch.optim.Adam(self.model.parameters(), lr = self.compile_params[\"initial_learning_rate\"])\n",
    "        self.scheduler = self.learning_rate_scheduler()\n",
    "    #\n",
    "    def learning_rate_scheduler(self): \n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size = self.compile_params[\"step_size\"], gamma = self.compile_params[\"gamma\"])\n",
    "        return(scheduler)\n",
    "    #\n",
    "    def write_training_statistics(self, epoch, average_train_loss, average_valid_loss, best_valid_loss, current_learning_rate):\n",
    "        te = time.time()\n",
    "        if epoch == 0:\n",
    "            self.t_previous = self.initial_time\n",
    "            header = \"Epoch\" + \"\\t\" + \"Train_loss\" + \"\\t\" + \"Validation_loss\" + \"\\t\" + \"Best_validation_loss \" + \"\\t\" + \"learning_rate\" + \"\\t\" + \"Computing_time\" + \"\\n\"\n",
    "            if os.path.isfile(self.filename_training_stats):\n",
    "                os.system(\"rm \" + self.filename_training_stats)\n",
    "            with open(self.filename_training_stats, \"w\") as stats_file:\n",
    "                stats_file.write(header)\n",
    "        #\n",
    "        time_epoch = round(te - self.t_previous)\n",
    "        self.t_previous = te\n",
    "        #\n",
    "        stats_str = str(epoch + 1) + \"\\t\" + \\\n",
    "                    str(average_train_loss) + \"\\t\" + \\\n",
    "                    str(average_valid_loss) + \"\\t\" + \\\n",
    "                    str(best_valid_loss) + \"\\t\" + \\\n",
    "                    str(current_learning_rate) + \"\\t\" + \\\n",
    "                    str(time_epoch) + \"\\n\"\n",
    "        #\n",
    "        with open(self.filename_training_stats, \"a\") as stats_file:\n",
    "            stats_file.write(stats_str)\n",
    "    #\n",
    "    def save_best_model(self, model, average_valid_loss, best_valid_loss, opt, epoch):\n",
    "        print(\"Validation loss improved from \" + str(best_valid_loss) + \" to \" + str(average_valid_loss) + \". Model saved.\")\n",
    "        saved_stats = {\"epoch\": epoch + 1,\n",
    "                       \"model_state_dict\": model.state_dict(),\n",
    "                       \"optimizer_state_dict\": opt.state_dict(),\n",
    "                       \"val_loss\": average_valid_loss}\n",
    "        #\n",
    "        torch.save(saved_stats, self.filename_model)\n",
    "    #\n",
    "    def training_loop(self):\n",
    "        best_valid_loss = float(\"inf\")\n",
    "        scaler = torch.amp.GradScaler()\n",
    "        #\n",
    "        for epoch in range(0, self.compile_params[\"n_epochs\"]):\n",
    "            print(\"epoch\", epoch)\n",
    "            #\n",
    "            # Training\n",
    "            #\n",
    "            self.model.train()  # Set model to training mode\n",
    "            train_loss = 0\n",
    "            total_samples_train = 0\n",
    "            #\n",
    "            for batch_train in self.train_loader:\n",
    "                data_batch = batch_train.to(self.device)\n",
    "                x, y, a, batch = data_batch.x, data_batch.y, data_batch.edge_index, data_batch.batch\n",
    "                #\n",
    "                self.optimizer.zero_grad()                       # Clear old gradients, and avoid accumulating gradients from previous batches\n",
    "                with torch.amp.autocast(device_type = \"cuda\"):   # This is used for mixed precision training\n",
    "                    output = self.model(x, a, batch)             # Make predictions\n",
    "                    loss = self.loss_function(output, y)         # Compute loss\n",
    "                scaler.scale(loss).backward()                    # Backward pass, compute new gradients\n",
    "                scaler.step(self.optimizer)                      # Update weights based on new gradients\n",
    "                scaler.update()                                  # Adjust the scaling factor for the next iteration \n",
    "                #\n",
    "                train_loss += loss.item() * y.size(0)\n",
    "                total_samples_train += y.size(0)\n",
    "            #\n",
    "            average_train_loss = train_loss / total_samples_train\n",
    "            #\n",
    "            # Validation\n",
    "            #\n",
    "            self.model.eval()\n",
    "            valid_loss = 0\n",
    "            total_samples_valid = 0\n",
    "            #\n",
    "            with torch.no_grad(), torch.amp.autocast(device_type = \"cuda\"):\n",
    "                for batch_valid in self.valid_loader:\n",
    "                    data_batch = batch_valid.to(self.device)\n",
    "                    x, y, a, batch = data_batch.x, data_batch.y, data_batch.edge_index, data_batch.batch\n",
    "                    #\n",
    "                    output = self.model(x, a, batch)                             # Make predictions\n",
    "                    loss = self.loss_function(output, y)                         # Compute loss\n",
    "                    valid_loss += loss.item() * y.size(0)                        # Accumulate loss over several batches with weighting the losses according to the number of graphs within the batch\n",
    "                    total_samples_valid += y.size(0)                             # Compute the total number of graphs from the beginning of the epoch \n",
    "            #\n",
    "            average_valid_loss = valid_loss / total_samples_valid                # Compute the mean loss for the epoch\n",
    "            current_learning_rate = self.optimizer.param_groups[0]['lr']         # Extract the current learning rate\n",
    "            self.scheduler.step()                                                # Update learning rate\n",
    "            #\n",
    "            if average_valid_loss < best_valid_loss:\n",
    "                self.save_best_model(model = self.model,\n",
    "                                     average_valid_loss = average_valid_loss,\n",
    "                                     best_valid_loss = best_valid_loss,\n",
    "                                     opt = self.optimizer,\n",
    "                                     epoch = epoch)\n",
    "                #\n",
    "                best_valid_loss = average_valid_loss\n",
    "            #\n",
    "            self.write_training_statistics(epoch = epoch,\n",
    "                                           average_train_loss = average_train_loss,\n",
    "                                           average_valid_loss = average_valid_loss,\n",
    "                                           best_valid_loss = best_valid_loss,\n",
    "                                           current_learning_rate = current_learning_rate)\n",
    "        #\n",
    "        return(self.model)\n",
    "    #\n",
    "    def __call__(self):\n",
    "        trained_model = self.training_loop()\n",
    "        return(trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching index: 16954, sample_id: 16955\n",
      "Fetching index: 19268, sample_id: 19269\n",
      "Fetching index: 18123, sample_id: 18124\n",
      "Fetching index: 7283, sample_id: 7284\n",
      "Fetching index: 364, sample_id: 365\n",
      "Fetching index: 6232, sample_id: 6233\n",
      "Fetching index: 5777, sample_id: 5778\n",
      "Fetching index: 18873, sample_id: 18874\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [41, 22] at entry 0 and [42, 22] at entry 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [133], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m model_params, train_loader, valid_loader \u001b[38;5;241m=\u001b[39m  make_parameters(paths \u001b[38;5;241m=\u001b[39m paths, \n\u001b[1;32m      2\u001b[0m                                                             filename_normalization \u001b[38;5;241m=\u001b[39m filename_normalization, \n\u001b[1;32m      3\u001b[0m                                                             AMSR2_frequency \u001b[38;5;241m=\u001b[39m AMSR2_frequency, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                             date_min_valid \u001b[38;5;241m=\u001b[39m date_min_valid, \n\u001b[1;32m     15\u001b[0m                                                             date_max_valid \u001b[38;5;241m=\u001b[39m date_max_valid)()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bi, batch_train \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Number of graphs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_train\u001b[38;5;241m.\u001b[39mnum_graphs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch_geometric/loader/dataloader.py:43\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, Mapping):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: \u001b[38;5;28mself\u001b[39m([data[key] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m batch]) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(elem, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_fields\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(elem)(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch_geometric/loader/dataloader.py:43\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, Mapping):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(elem, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_fields\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(elem)(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch_geometric/loader/dataloader.py:33\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Batch\u001b[38;5;241m.\u001b[39mfrom_data_list(\n\u001b[1;32m     28\u001b[0m         batch,\n\u001b[1;32m     29\u001b[0m         follow_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_batch,\n\u001b[1;32m     30\u001b[0m         exclude_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclude_keys,\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, TensorFrame):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_frame\u001b[38;5;241m.\u001b[39mcat(batch, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [41, 22] at entry 0 and [42, 22] at entry 2"
     ]
    }
   ],
   "source": [
    "model_params, train_loader, valid_loader =  make_parameters(paths = paths, \n",
    "                                                            filename_normalization = filename_normalization, \n",
    "                                                            AMSR2_frequency = AMSR2_frequency, \n",
    "                                                            AMSR2_footprint_radius = AMSR2_footprint_radius,\n",
    "                                                            predictors = predictors, \n",
    "                                                            activation = activation, \n",
    "                                                            weight_initializer = weight_initializer, \n",
    "                                                            conv_filters = conv_filers, \n",
    "                                                            batch_size = batch_size, \n",
    "                                                            batch_normalization = batch_normalization,\n",
    "                                                            attention_heads = attention_heads,\n",
    "                                                            shuffle = shuffle, \n",
    "                                                            date_min_train = date_min_train, \n",
    "                                                            date_max_train = date_max_train, \n",
    "                                                            date_min_valid = date_min_valid, \n",
    "                                                            date_max_valid = date_max_valid,\n",
    "                                                            subsampling = subsampling)()\n",
    "#\n",
    "GNN_model = GNN_GAT(**model_params).to(device)\n",
    "#\n",
    "save_model_details(model = GNN_model, \n",
    "                   paths = paths, \n",
    "                   weight_initializer_str = weight_initializer_str, \n",
    "                   shuffle = shuffle, \n",
    "                   batch_normalization = batch_normalization, \n",
    "                   attention_heads = attention_heads,\n",
    "                   batch_size = batch_size, \n",
    "                   compile_params = compile_params, \n",
    "                   predictors = predictors, \n",
    "                   AMSR2_frequency = AMSR2_frequency, \n",
    "                   date_min_train = date_min_train, \n",
    "                   date_max_train = date_max_train, \n",
    "                   date_min_valid = date_min_valid, \n",
    "                   date_max_valid = date_max_valid,\n",
    "                   subsampling = subsampling)\n",
    "#\n",
    "trained_model = train_model(AMSR2_frequency = AMSR2_frequency,\n",
    "                            model = GNN_model, \n",
    "                            train_loader = train_loader, \n",
    "                            valid_loader = valid_loader, \n",
    "                            compile_params = compile_params, \n",
    "                            paths = paths,\n",
    "                            filename_normalization = filename_normalization,\n",
    "                            device = device,\n",
    "                            initial_time = t0)()\n",
    "#\n",
    "tf = time.time()\n",
    "print(\"Computing time\", tf - t0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
