{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import h5py\n",
    "import scipy\n",
    "import pyproj\n",
    "import netCDF4\n",
    "import datetime\n",
    "import pyresample\n",
    "import numpy as np\n",
    "#\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGE_TASK_ID = 1\n",
    "#\n",
    "date_min = \"20200901\"\n",
    "date_max = \"20230602\"\n",
    "#\n",
    "AMSR2_task_frequency = \"18.7\"\n",
    "AMSR2_frequencies = [\"6.9\", \"7.3\", \"10.7\", \"18.7\", \"23.8\", \"36.5\"]\n",
    "AMSR2_footprint_radius = [0.25 * (35 + 62), 0.25 * (35 + 62), 0.25 * (24 + 42), 0.25 * (14 + 22), 0.25 * (11 + 19), 0.25 * (7 + 12)]  # 0.5 * mean diameter (0.5 * (major + minor))\n",
    "#\n",
    "paths = {}\n",
    "paths[\"AMSR2\"] = \"/lustre/storeB/immutable/archive/projects/remotesensing/satellite/\"\n",
    "paths[\"MEPS\"] = \"/lustre/storeB/immutable/archive/projects/metproduction/meps/\"\n",
    "paths[\"surfex\"] = \"/lustre/storeB/users/josteinbl/sfx_data/LDAS_NOR/archive/\"\n",
    "paths[\"surfex_grid\"] = \"/lustre/storeB/users/josteinbl/sfx_data/LDAS_NOR/climate/\"\n",
    "paths[\"output\"] = \"/lustre/storeB/project/nwp/H2O/wp3/Deep_learning_predictions/Training_data_GNN/\" + AMSR2_task_frequency.split('.')[0] + \"GHz_static/\"\n",
    "#\n",
    "hours_AMSR2 = \"H03\"\n",
    "MEPS_leadtime = int(hours_AMSR2[2])\n",
    "#\n",
    "crs = {}\n",
    "crs[\"latlon\"] = pyproj.CRS.from_proj4(\"+proj=latlon\")\n",
    "#\n",
    "MEPS_spatial_resolution = 2500\n",
    "MEPS_dim_variables = [\"time\", \"pressure\", \"x\", \"y\", \"longitude\", \"latitude\"]\n",
    "MEPS_LWE_thickness = [\"lwe_thickness_of_atmosphere_mass_content_of_water_vapor\"]\n",
    "MEPS_clouds = [\"mass_fraction_of_cloud_condensed_water_in_air_pl\", \"mass_fraction_of_cloud_ice_in_air_pl\", ]\n",
    "MEPS_precip = [\"mass_fraction_of_snow_in_air_pl\", \"mass_fraction_of_rain_in_air_pl\", \"mass_fraction_of_graupel_in_air_pl\"]\n",
    "MEPS_specific_humidity = [\"specific_humidity_pl\"]\n",
    "MEPS_PL_variables = MEPS_LWE_thickness + MEPS_clouds + MEPS_precip + MEPS_specific_humidity\n",
    "#\n",
    "surfex_PGD_variables = [\"ZS\", \"COVER004\", \"COVER006\"]\n",
    "surfex_prognostic_variables = [\"FRAC_WATER\", \"FRAC_NATURE\", \"FRAC_SEA\"]\n",
    "surfex_surface_and_integrated_variables = [\"Q2M_ISBA\", \"T2M_ISBA\", \"TS_ISBA\", \"DSN_T_ISBA\", \"LAI_ga\", \"PSN_ISBA\", \"PSNG_ISBA\", \"PSNV_ISBA\"]\n",
    "predictor_variables = surfex_prognostic_variables + surfex_surface_and_integrated_variables\n",
    "#\n",
    "n_soil_layers = 2\n",
    "n_snow_layers = 12\n",
    "surfex_soil_variables = [\"TG\", \"WSN_VEG\", \"WG\", \"WGI\", \"RSN_VEG\", \"SNOWTEMP\", \"SNOWLIQ\", \"HSN_VEG\"] \n",
    "#\n",
    "for var in surfex_soil_variables:\n",
    "    if (\"SNOW\" in var) or (\"_VEG\" in var):\n",
    "        for layer in range(1, n_snow_layers + 1):\n",
    "            predictor_variables.append(var + str(layer) + \"_ga\")\n",
    "    else:\n",
    "        for layer in range(1, n_soil_layers + 1):\n",
    "            predictor_variables.append(var + str(layer) + \"_ga\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list_dates(date_min, date_max):\n",
    "    current_date = datetime.datetime.strptime(date_min, \"%Y%m%d\")\n",
    "    end_date = datetime.datetime.strptime(date_max, \"%Y%m%d\")\n",
    "    list_dates = []\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime(\"%Y%m%d\")\n",
    "        list_dates.append(date_str)\n",
    "        current_date = current_date + datetime.timedelta(days = 1)\n",
    "    return(list_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Surfex coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_surfex_coordinates():\n",
    "    def __init__(self, crs, surfex_PGD_variables, paths):\n",
    "        self.crs = crs\n",
    "        self.surfex_PGD_variables = surfex_PGD_variables\n",
    "        self.inputgrid = paths[\"surfex_grid\"] + \"PGD.nc\"\n",
    "    #\n",
    "    def sfx2areadef(self, lat0, lon0, latori, lonori, xx, yy):\n",
    "        proj2 = \"+proj=lcc +lat_1=%.2f +lat_2=%.2f +lat_0=%.2f +lon_0=%.2f +units=m +ellps=WGS84 +no_defs\" % (lat0,lat0,lat0,lon0)\n",
    "        p2 = pyproj.Proj(proj2, preserve_units = False)\n",
    "        origo = p2(lonori.data, latori.data)\n",
    "        extent = origo + (origo[0] + xx[-1,-1], origo[1] + yy[-1,-1])\n",
    "        area_def = pyresample.geometry.AreaDefinition(\"id2\", \"hei2\", \"lcc\", proj2, xx.shape[1], yy.shape[0], extent)\n",
    "        return(area_def)\n",
    "    #\n",
    "    def getSFXgrid(self):\n",
    "        with netCDF4.Dataset(self.inputgrid, \"r\") as nc:\n",
    "            areadef = self.sfx2areadef(lat0 = nc[\"LAT0\"][0], lon0 = nc[\"LON0\"][0], latori = nc[\"LATORI\"][0], lonori = nc[\"LONORI\"][0], xx = nc[\"XX\"][:], yy = nc[\"YY\"][:])\n",
    "        return(areadef)\n",
    "    #\n",
    "    def load_PGD_variables(self):\n",
    "        Surfex_PGD = {}\n",
    "        with netCDF4.Dataset(self.inputgrid, \"r\") as nc:\n",
    "            for var in self.surfex_PGD_variables:\n",
    "                Surfex_PGD[var] = np.flipud(nc.variables[var][:,:])\n",
    "        return(Surfex_PGD)\n",
    "    #\n",
    "    def __call__(self):\n",
    "        areadef = self.getSFXgrid()\n",
    "        #\n",
    "        lon, lat = areadef.get_lonlats()\n",
    "        #\n",
    "        Surfex_PGD = self.load_PGD_variables()\n",
    "        #\n",
    "        Surfex_coord = {}\n",
    "        Surfex_coord[\"proj4_string\"] = areadef.proj4_string\n",
    "        Surfex_coord[\"crs\"] = areadef.crs\n",
    "        Surfex_coord[\"lon\"] = lon\n",
    "        Surfex_coord[\"lat\"] = lat\n",
    "        #\n",
    "        transform_to_surfex = pyproj.Transformer.from_crs(self.crs[\"latlon\"], Surfex_coord[\"crs\"], always_xy = True)\n",
    "        Surfex_coord[\"xx\"], Surfex_coord[\"yy\"] = transform_to_surfex.transform(Surfex_coord[\"lon\"], Surfex_coord[\"lat\"])\n",
    "        Surfex_coord[\"x\"] = Surfex_coord[\"xx\"][0,:]\n",
    "        Surfex_coord[\"y\"] = Surfex_coord[\"yy\"][:,0]\n",
    "        Surfex_coord[\"x_min\"] = np.min(Surfex_coord[\"x\"])\n",
    "        Surfex_coord[\"x_max\"] = np.max(Surfex_coord[\"x\"])\n",
    "        Surfex_coord[\"y_min\"] = np.min(Surfex_coord[\"y\"])\n",
    "        Surfex_coord[\"y_max\"] = np.max(Surfex_coord[\"y\"])\n",
    "        #\n",
    "        return(Surfex_coord, Surfex_PGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read SURFEX data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_surfex_data(date_task_hours_AMSR2, paths, predictor_variables):\n",
    "    Surfex_data = {}\n",
    "    filename_constants = paths[\"surfex\"] + \"2023/01/01/00/SURFOUT.20230101_03h00.nc\"\n",
    "    previous_hours = \"{:02d}\".format(int(date_task_hours_AMSR2[9:11]) - 3)\n",
    "    path_task = paths[\"surfex\"] + date_task_hours_AMSR2[0:4] + \"/\" + date_task_hours_AMSR2[4:6] + \"/\" + date_task_hours_AMSR2[6:8] + \"/\" +  previous_hours + \"/\"\n",
    "    #\n",
    "    with netCDF4.Dataset(path_task + \"SURFOUT.\" + date_task_hours_AMSR2[0:8] + \"_\" + date_task_hours_AMSR2[9:11] + \"h00.nc\", \"r\") as nc:\n",
    "        for var in [\"PATCHP1\", \"PATCHP2\"]:\n",
    "            if var in nc.variables:\n",
    "                var_data = np.flipud(nc.variables[var][:,:])\n",
    "                var_data[var_data.mask == True] = 0\n",
    "                Surfex_data[var] = np.expand_dims(var_data, axis = 0)\n",
    "            else:\n",
    "                with netCDF4.Dataset(filename_constants, \"r\") as nc_constants:\n",
    "                    var_data = np.flipud(nc_constants.variables[var][:,:])\n",
    "                    var_data[var_data.mask == True] = 0\n",
    "                    Surfex_data[var] = np.expand_dims(var_data, axis = 0)\n",
    "        #\n",
    "        for var in predictor_variables:\n",
    "            try:\n",
    "                if (var in nc.variables) or (var.replace(\"_ga\", \"P1\") in nc.variables):\n",
    "                    if \"_ga\" in var:\n",
    "                        var_data_P1 = np.flipud(nc.variables[var.replace(\"_ga\", \"P1\")][:,:])\n",
    "                        var_data_P2 = np.flipud(nc.variables[var.replace(\"_ga\", \"P2\")][:,:])\n",
    "                        var_data_P1 = np.expand_dims(var_data_P1, axis = 0)\n",
    "                        var_data_P2 = np.expand_dims(var_data_P2, axis = 0)\n",
    "                        Surfex_data[var] = np.nansum([Surfex_data[\"PATCHP1\"] * var_data_P1, Surfex_data[\"PATCHP2\"] * var_data_P2], axis = 0)\n",
    "                        Surfex_data[var][var_data_P1.mask == True] = var_data_P2[var_data_P1.mask == True]\n",
    "                        Surfex_data[var][var_data_P2.mask == True] = var_data_P1[var_data_P2.mask == True]\n",
    "                        Surfex_data[var][np.logical_and(var_data_P1.mask == True, var_data_P2.mask == True)] = np.nan\n",
    "                    else:\n",
    "                        Surfex_data[var] = np.flipud(nc.variables[var][:,:])\n",
    "                else:\n",
    "                    with netCDF4.Dataset(path_task + \"SURFOUT.nc\", \"r\") as ncp:\n",
    "                        if \"_ga\" in var:\n",
    "                            var_data_P1 = np.flipud(ncp.variables[var.replace(\"_ga\", \"P1\")][:,:])\n",
    "                            var_data_P2 = np.flipud(ncp.variables[var.replace(\"_ga\", \"P2\")][:,:])\n",
    "                            var_data_P1 = np.expand_dims(var_data_P1, axis = 0)\n",
    "                            var_data_P2 = np.expand_dims(var_data_P2, axis = 0)\n",
    "                            Surfex_data[var] = np.nansum([Surfex_data[\"PATCHP1\"] * var_data_P1, Surfex_data[\"PATCHP2\"] * var_data_P2], axis = 0)\n",
    "                            Surfex_data[var][var_data_P1.mask == True] = var_data_P2[var_data_P1.mask == True]\n",
    "                            Surfex_data[var][var_data_P2.mask == True] = var_data_P1[var_data_P2.mask == True]\n",
    "                            Surfex_data[var][np.logical_and(var_data_P1.mask == True, var_data_P2.mask == True)] = np.nan\n",
    "                        else:\n",
    "                            Surfex_data[var] = np.flipud(ncp.variables[var][:,:])\n",
    "            except:\n",
    "                print(\"Variable not found: \" + var)\n",
    "                if (var == \"SNOWTEMP9_ga\") or (var == \"SNOWLIQ9_ga\"):\n",
    "                    pass\n",
    "                else:\n",
    "                    sys.exit()\n",
    "    #\n",
    "    return(Surfex_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WSN_T_ISBA(Surfex_data, n_soil_layers):\n",
    "    WSN_T_ISBA = np.zeros(np.shape(Surfex_data[\"WSN_VEG1_ga\"]))\n",
    "    for layer in range(0, n_soil_layers):\n",
    "        WSN_T_ISBA = WSN_T_ISBA + Surfex_data[\"WSN_VEG\" + str(layer + 1) + \"_ga\"]\n",
    "    WSN_T_ISBA[WSN_T_ISBA > 1e10] = np.nan\n",
    "    return(WSN_T_ISBA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get MEPS variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_MEPS_data():\n",
    "    def __init__(self, date_task, MEPS_leadtime, MEPS_dim_variables, MEPS_PL_variables, Surfex_coord, crs, paths):\n",
    "        self.date_task = date_task\n",
    "        self.MEPS_leadtime = MEPS_leadtime\n",
    "        self.MEPS_dim_variables = MEPS_dim_variables\n",
    "        self.MEPS_PL_variables = MEPS_PL_variables\n",
    "        self.Surfex_coord = Surfex_coord\n",
    "        self.crs = crs\n",
    "        self.paths = paths\n",
    "        self.filename = paths[\"MEPS\"] + date_task[0:4] + \"/\" + date_task[4:6] + \"/\" + date_task[6:8] + \"/\" + \"meps_det_2_5km_\" + date_task + \"T00Z.nc\"\n",
    "    #\n",
    "    def nearest_neighbor_indexes(self, x_input, y_input, x_output, y_output):\n",
    "        # x_input, y_input, x_output, and y_output must be vectors\n",
    "        x_input = np.expand_dims(x_input, axis = 1)\n",
    "        y_input = np.expand_dims(y_input, axis = 1)\n",
    "        x_output = np.expand_dims(x_output, axis = 1)\n",
    "        y_output = np.expand_dims(y_output, axis = 1)\n",
    "        #\n",
    "        coord_input = np.concatenate((x_input, y_input), axis = 1)\n",
    "        coord_output = np.concatenate((x_output, y_output), axis = 1)\n",
    "        #\n",
    "        tree = scipy.spatial.KDTree(coord_input)\n",
    "        dist, idx = tree.query(coord_output)\n",
    "        #\n",
    "        return(idx)\n",
    "    #\n",
    "    def calculate_vertically_integrated_variable(self, pressure_levels, var_3D):\n",
    "        diff_pressure_extend = np.diff(pressure_levels)[:, np.newaxis, np.newaxis]\n",
    "        integrated_var = np.sum(diff_pressure_extend * var_3D[1:len(pressure_levels),:,:], axis = 0)\n",
    "        return(integrated_var)\n",
    "    #\n",
    "    def load_data(self):\n",
    "        Dataset = {}\n",
    "        if os.path.isfile(self.filename) == True:\n",
    "            with netCDF4.Dataset(self.filename, \"r\") as nc:\n",
    "                #\n",
    "                for var in self.MEPS_dim_variables:\n",
    "                    Dataset[var] = nc.variables[var][:]\n",
    "                #\n",
    "                for var in self.MEPS_PL_variables:\n",
    "                    if \"_pl\" in var:\n",
    "                        if nc.variables[var][:].ndim == 4:\n",
    "                            output_var_name = (\"vertically_integrated_\" + var).replace(\"_pl\", \"\")\n",
    "                            var_data = nc.variables[var][self.MEPS_leadtime,:,:,:]\n",
    "                            Dataset[output_var_name] = self.calculate_vertically_integrated_variable(Dataset[\"pressure\"], var_data)\n",
    "                    #\n",
    "                    if var == \"lwe_thickness_of_atmosphere_mass_content_of_water_vapor\":\n",
    "                        Dataset[var] = np.squeeze(nc.variables[var][self.MEPS_leadtime,:,:,:])\n",
    "        return(Dataset)   \n",
    "    #\n",
    "    def projecting_MEPS_data_onto_Surfex_domain(self, Dataset):\n",
    "        Dataset_on_Surfex_grid = {}\n",
    "        transform_to_surfex = pyproj.Transformer.from_crs(self.crs[\"latlon\"], self.Surfex_coord[\"crs\"], always_xy = True)\n",
    "        xx_surfex = np.ndarray.flatten(self.Surfex_coord[\"xx\"])\n",
    "        yy_surfex = np.ndarray.flatten(self.Surfex_coord[\"yy\"])\n",
    "        xx_MEPS_on_Surfex_grid, yy_MEPS_on_Surfex_grid = transform_to_surfex.transform(Dataset[\"longitude\"], Dataset[\"latitude\"])\n",
    "        xx_MEPS_on_Surfex_grid = np.ndarray.flatten(xx_MEPS_on_Surfex_grid)\n",
    "        yy_MEPS_on_Surfex_grid = np.ndarray.flatten(yy_MEPS_on_Surfex_grid)\n",
    "        idx_MEPS_to_Surfex = self.nearest_neighbor_indexes(xx_MEPS_on_Surfex_grid, yy_MEPS_on_Surfex_grid, xx_surfex, yy_surfex)\n",
    "        #\n",
    "        for var in Dataset:\n",
    "            if var not in self.MEPS_dim_variables:\n",
    "                field_flat = np.ndarray.flatten(Dataset[var])\n",
    "                field_interp = field_flat[idx_MEPS_to_Surfex]\n",
    "                Dataset_on_Surfex_grid[var] = np.reshape(field_interp, (len(self.Surfex_coord[\"y\"]), len(self.Surfex_coord[\"x\"])), order = \"C\")\n",
    "        #\n",
    "        return(Dataset_on_Surfex_grid)\n",
    "    #\n",
    "    def __call__(self):\n",
    "        Dataset = self.load_data()\n",
    "        Dataset_on_Surfex_grid = self.projecting_MEPS_data_onto_Surfex_domain(Dataset)\n",
    "        return(Dataset_on_Surfex_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read AMSR2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class read_AMSR2_data():\n",
    "    def __init__(self, filename, AMSR2_task_frequency):\n",
    "        self.filename = filename\n",
    "        self.AMSR2_task_frequency = AMSR2_task_frequency\n",
    "    #\n",
    "    def decode_CoRegistrationParameter(self, Astr):\n",
    "        # from https://gitlab.met.no/met/obsklim/satellitt/microwave-sdr-to-osisaf-nc/-/blob/master/microwave_to_osisaf_nc/amsr2h5_l1b_reader.py\n",
    "        ch_coreg = Astr.split(',')\n",
    "        if len(ch_coreg) != 6:\n",
    "            raise ValueError('Did not find expected 6 channels in co-registration parameter list')\n",
    "        #\n",
    "        coreg = dict()\n",
    "        for ch in ch_coreg:\n",
    "            try:\n",
    "                ch_nam, ch_reg = ch.split('-', 1)\n",
    "                ch_reg = float(ch_reg)\n",
    "            except Exception as e:\n",
    "                raise ValueError('Wrong format for the CoRegistration string %s (%s)' % (ch, e,))\n",
    "            coreg[ch_nam] = ch_reg\n",
    "        return(coreg)\n",
    "    #\n",
    "    def get_low_freq_latlon(self, lat89A, lon89A, A1, A2):\n",
    "        # from https://gitlab.met.no/met/obsklim/satellitt/microwave-sdr-to-osisaf-nc/-/blob/master/microwave_to_osisaf_nc/amsr2h5_l1b_reader.py\n",
    "        # compute lat/lon for a low-frequency channel from the 89A lat/lon\n",
    "        #    (ref: AMSR2_Level1_Product_Format_EN.pdf page 4-16,4-17)\n",
    "        # split the 89A positions in odd and even.\n",
    "        #    WARNING: the numbering in the AMSR2 PUM is from 1, so their\n",
    "        #                'even' (2,4,6,...) are (1,3,5,...) for Python\n",
    "        evn_lat89A = np.deg2rad(lat89A[:, 1::2])\n",
    "        evn_lon89A = np.deg2rad(lon89A[:, 1::2])\n",
    "        odd_lat89A = np.deg2rad(lat89A[:, 0::2])\n",
    "        odd_lon89A = np.deg2rad(lon89A[:, 0::2])\n",
    "        # transform to Cartesian x,y,z coordinates\n",
    "        P1 = np.dstack((np.cos(odd_lon89A) * np.cos(odd_lat89A), np.sin(odd_lon89A) * np.cos(odd_lat89A), np.sin(odd_lat89A)))\n",
    "        P2 = np.dstack((np.cos(evn_lon89A) * np.cos(evn_lat89A), np.sin(evn_lon89A) * np.cos(evn_lat89A), np.sin(evn_lat89A)))\n",
    "        # Get orthogonal base Ex,Ey,Ez\n",
    "        Ex = P1\n",
    "        Ez = np.cross(P1, P2)\n",
    "        EzNorm = ((Ez ** 2).sum(axis = 2))**0.5\n",
    "        for d in range(3):\n",
    "            Ez[:, :, d] /= EzNorm\n",
    "        Ey = np.cross(Ez, Ex)\n",
    "        # Get Theta: angle between two consecutives 89A positions along the scan\n",
    "        Theta = np.arccos((P1 * P2).sum(axis = 2))\n",
    "        # Compute cartesian position of low-frequency channel with A1 and A2\n",
    "        cA2T = np.cos(A2 * Theta)\n",
    "        cA1T = np.cos(A1 * Theta)\n",
    "        sA1T = np.sin(A1 * Theta)\n",
    "        sA2T = np.sin(A2 * Theta)\n",
    "        Pt = np.empty_like(Ex)\n",
    "        for d in range(3):\n",
    "            Pt[:, :, d] = cA2T * (cA1T * Ex[:, :, d] + sA1T * Ey[:, :, d]) + sA2T * Ez[:, :, d]\n",
    "        # Transform back from Cartersian to Lat/Lon\n",
    "        Lat = np.arcsin(Pt[:, :, 2])\n",
    "        Lon = np.arctan2(Pt[:, :, 1], Pt[:, :, 0])\n",
    "        return(np.rad2deg(Lat), np.rad2deg(Lon)) \n",
    "    #\n",
    "    def __call__(self):\n",
    "        AMSR2_dataset = {}\n",
    "        with h5py.File(self.filename, \"r\") as hdf:\n",
    "            lat89A = hdf[\"Latitude of Observation Point for 89A\"][()]\n",
    "            lon89A = hdf[\"Longitude of Observation Point for 89A\"][()]\n",
    "            coreg_str_A1 = hdf.attrs[\"CoRegistrationParameterA1\"][0]\n",
    "            coreg_str_A2 = hdf.attrs[\"CoRegistrationParameterA2\"][0]\n",
    "            coreg_A1 = self.decode_CoRegistrationParameter(coreg_str_A1)\n",
    "            coreg_A2 = self.decode_CoRegistrationParameter(coreg_str_A2)\n",
    "            A1 = coreg_A1[self.AMSR2_task_frequency.split(\".\")[0] + \"G\"]\n",
    "            A2 = coreg_A2[self.AMSR2_task_frequency.split(\".\")[0] + \"G\"]\n",
    "            lat_task_freq, lon_task_freq = self.get_low_freq_latlon(lat89A, lon89A, A1, A2)\n",
    "            AMSR2_dataset[\"lat\"] = lat_task_freq\n",
    "            AMSR2_dataset[\"lon\"] = lon_task_freq\n",
    "            AMSR2_dataset[\"BT\" + self.AMSR2_task_frequency + \"H\"] = hdf[\"Brightness Temperature (\" + self.AMSR2_task_frequency + \"GHz,H)\"][()] * 0.01  # 0.01 = Scaling factor\n",
    "            AMSR2_dataset[\"BT\" + self.AMSR2_task_frequency + \"V\"] = hdf[\"Brightness Temperature (\" + self.AMSR2_task_frequency + \"GHz,V)\"][()] * 0.01\n",
    "            AMSR2_dataset[\"BT\" + self.AMSR2_task_frequency + \"H\"][AMSR2_dataset[\"BT\" + self.AMSR2_task_frequency + \"H\"] > 600] = np.nan\n",
    "            AMSR2_dataset[\"BT\" + self.AMSR2_task_frequency + \"V\"][AMSR2_dataset[\"BT\" + self.AMSR2_task_frequency + \"V\"] > 600] = np.nan\n",
    "        return(AMSR2_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extract_graphs():\n",
    "    def __init__(self, date_task, paths, AMSR2_task_frequency, static_dimension, crs, Surfex_coord, Surfex_data, MEPS_data):\n",
    "        self.date_task = date_task\n",
    "        self.paths = paths\n",
    "        self.AMSR2_task_frequency = AMSR2_task_frequency\n",
    "        self.static_dimension = static_dimension\n",
    "        self.crs = crs\n",
    "        self.Surfex_coord = Surfex_coord\n",
    "        self.Surfex_data = Surfex_data\n",
    "        self.MEPS_data = MEPS_data\n",
    "        self.min_number_of_obs = 100\n",
    "        self.max_hour = 6 # 6 AM\n",
    "        self.transform_to_surfex = pyproj.Transformer.from_crs(self.crs[\"latlon\"], self.Surfex_coord[\"crs\"], always_xy = True)\n",
    "        self.path_date = paths[\"AMSR2\"] + self.date_task[0:4] + \"/\" + self.date_task[4:6] + \"/\" + self.date_task[6:8] + \"/amsr2/jaxa/\"\n",
    "        self.dataset = sorted(glob.glob(self.path_date + \"GW1AM2_\" + self.date_task + \"0*.h5\"))\n",
    "    #\n",
    "    def AMSR2_on_surfex_domain(self, AMSR2_dataset):\n",
    "        AMSR2_xx, AMSR2_yy = self.transform_to_surfex.transform(AMSR2_dataset[\"lon\"], AMSR2_dataset[\"lat\"])\n",
    "        idx_x = np.logical_and(AMSR2_xx > self.Surfex_coord[\"x_min\"], AMSR2_xx < self.Surfex_coord[\"x_max\"])\n",
    "        idx_y = np.logical_and(AMSR2_yy > self.Surfex_coord[\"y_min\"], AMSR2_yy < self.Surfex_coord[\"y_max\"])\n",
    "        idx_domain = np.logical_and(idx_x == True, idx_y == True)\n",
    "        N_obs = np.sum(idx_domain == True)\n",
    "        #\n",
    "        AMSR2_surfex_domain = {}\n",
    "        if N_obs > self.min_number_of_obs:\n",
    "            AMSR2_surfex_domain[\"N_obs\"] = N_obs\n",
    "            AMSR2_surfex_domain[\"xx\"] = AMSR2_xx[idx_domain == True]\n",
    "            AMSR2_surfex_domain[\"yy\"] = AMSR2_yy[idx_domain == True]\n",
    "            AMSR2_surfex_domain[\"lat\"] = AMSR2_dataset[\"lat\"][idx_domain == True]\n",
    "            AMSR2_surfex_domain[\"lon\"] = AMSR2_dataset[\"lon\"][idx_domain == True]\n",
    "            AMSR2_surfex_domain[\"BT\" + self.AMSR2_task_frequency + \"H\"] = AMSR2_dataset[\"BT\" + self.AMSR2_task_frequency + \"H\"][idx_domain == True]\n",
    "            AMSR2_surfex_domain[\"BT\" + self.AMSR2_task_frequency + \"V\"] = AMSR2_dataset[\"BT\" + self.AMSR2_task_frequency + \"V\"][idx_domain == True]\n",
    "        return(AMSR2_surfex_domain)\n",
    "    #\n",
    "    def calculate_distances(self, AMSR2_surfex_domain):\n",
    "        AMSR2_grid_points = np.stack([AMSR2_surfex_domain[\"xx\"], AMSR2_surfex_domain[\"yy\"]], axis = 1)\n",
    "        Surfex_grid_points = np.stack([np.ndarray.flatten(Surfex_coord[\"xx\"]), np.ndarray.flatten(self.Surfex_coord[\"yy\"])], axis = 1)\n",
    "        Distances_to_footprint_center = scipy.spatial.distance.cdist(AMSR2_grid_points, Surfex_grid_points, metric = \"euclidean\")\n",
    "        return(Distances_to_footprint_center)\n",
    "    #\n",
    "    def get_graph_data(self, Distances_to_footprint_center, AMSR2_surfex_domain):\n",
    "        Graphs = {}\n",
    "        for i in range(0, len(AMSR2_surfex_domain[\"xx\"])):\n",
    "            if (np.isnan(AMSR2_surfex_domain[\"BT\" + self.AMSR2_task_frequency + \"H\"][i]) == False) and (np.isnan(AMSR2_surfex_domain[\"BT\" + self.AMSR2_task_frequency + \"V\"][i]) == False):\n",
    "                Footprint_dist = np.argsort(Distances_to_footprint_center[i,:])[0:self.static_dimension]\n",
    "                xx_coord = np.ndarray.flatten(self.Surfex_coord[\"xx\"])[Footprint_dist]\n",
    "                yy_coord = np.ndarray.flatten(self.Surfex_coord[\"yy\"])[Footprint_dist]\n",
    "                Graph_coord = np.concatenate((np.expand_dims(xx_coord, axis = 1), np.expand_dims(yy_coord, axis = 1)), axis = 1)\n",
    "                #\n",
    "                if len(Graphs) == 0:\n",
    "                    Graphs[\"Distance_to_footprint_center\"] = np.expand_dims(Distances_to_footprint_center[i,:][Footprint_dist], axis = 0)\n",
    "                    Graphs[\"xx\"] = np.expand_dims(xx_coord, axis = 0)\n",
    "                    Graphs[\"yy\"] = np.expand_dims(yy_coord, axis = 0)\n",
    "                    Graphs[\"Distance_matrix\"] = np.expand_dims(scipy.spatial.distance.cdist(Graph_coord, Graph_coord, metric = \"euclidean\"), axis = 0)\n",
    "                    for var in self.Surfex_data:\n",
    "                        Graphs[var] = np.expand_dims(np.ndarray.flatten(self.Surfex_data[var])[Footprint_dist], axis = 0)\n",
    "                        if \"ISBA\" in var:\n",
    "                            Graphs[var][Graphs[var] > 1e10] = np.nan\n",
    "                    for var in self.MEPS_data:\n",
    "                        Graphs[var] = np.expand_dims(np.ndarray.flatten(self.MEPS_data[var])[Footprint_dist], axis = 0)\n",
    "                    for var in AMSR2_surfex_domain:\n",
    "                        if var != \"N_obs\":\n",
    "                            Graphs[\"AMSR2_\" + var] = np.expand_dims(AMSR2_surfex_domain[var][i], axis = 0)\n",
    "                else:\n",
    "                    Graphs[\"Distance_to_footprint_center\"] = np.concatenate((Graphs[\"Distance_to_footprint_center\"], np.expand_dims(Distances_to_footprint_center[i,:][Footprint_dist], axis = 0)), axis = 0)\n",
    "                    Graphs[\"xx\"] = np.concatenate((Graphs[\"xx\"], np.expand_dims(xx_coord, axis = 0)), axis = 0)\n",
    "                    Graphs[\"yy\"] = np.concatenate((Graphs[\"yy\"], np.expand_dims(yy_coord, axis = 0)), axis = 0)\n",
    "                    Graphs[\"Distance_matrix\"] = np.concatenate((Graphs[\"Distance_matrix\"], np.expand_dims(scipy.spatial.distance.cdist(Graph_coord, Graph_coord, metric = \"euclidean\"), axis = 0)), axis = 0)\n",
    "                    for var in self.Surfex_data:\n",
    "                        Graphs[var] = np.concatenate((Graphs[var], np.expand_dims(np.ndarray.flatten(self.Surfex_data[var])[Footprint_dist], axis = 0)), axis = 0)\n",
    "                        if \"ISBA\" in var:\n",
    "                            Graphs[var][Graphs[var] > 1e10] = np.nan\n",
    "                    for var in self.MEPS_data:\n",
    "                        Graphs[var] = np.concatenate((Graphs[var], np.expand_dims(np.ndarray.flatten(self.MEPS_data[var])[Footprint_dist], axis = 0)), axis = 0)\n",
    "                    for var in AMSR2_surfex_domain:\n",
    "                        if var != \"N_obs\":\n",
    "                            Graphs[\"AMSR2_\" + var] = np.concatenate((Graphs[\"AMSR2_\" + var], np.expand_dims(AMSR2_surfex_domain[var][i], axis = 0)), axis = 0)\n",
    "        return(Graphs)    \n",
    "    #\n",
    "    def write_graphs(self, Graphs):\n",
    "        path_output = self.paths[\"output\"] + self.date_task[0:4] + \"/\" + self.date_task[4:6] + \"/\"\n",
    "        if os.path.exists(path_output) == False:\n",
    "            os.system(\"mkdir -p \" + path_output) \n",
    "        filename = path_output + \"Graphs_\" + self.date_task + \".h5\"\n",
    "        #\n",
    "        with h5py.File(filename, \"a\") as hdf:\n",
    "            for var in Graphs:\n",
    "                if var in hdf:\n",
    "                    hdf_var = np.array(hdf[var][:])\n",
    "                    var_conc = np.concatenate((hdf_var, Graphs[var]), axis=0)\n",
    "                    del hdf[var]\n",
    "                    hdf.create_dataset(var, data = var_conc) \n",
    "                else:\n",
    "                    hdf.create_dataset(var, data = Graphs[var])\n",
    "    #\n",
    "    def __call__(self):\n",
    "        Graphs = {}\n",
    "        for filename in self.dataset:\n",
    "            hour_filename = int(filename[-29:-27])\n",
    "            if hour_filename < self.max_hour:\n",
    "                AMSR2_dataset = read_AMSR2_data(filename, self.AMSR2_task_frequency)()\n",
    "                AMSR2_surfex_domain = self.AMSR2_on_surfex_domain(AMSR2_dataset)\n",
    "                if len(AMSR2_surfex_domain) > 0:\n",
    "                    print(filename)\n",
    "                    Distances_to_footprint_center =  self.calculate_distances(AMSR2_surfex_domain)\n",
    "                    Graphs = self.get_graph_data(Distances_to_footprint_center, AMSR2_surfex_domain)\n",
    "                    self.write_graphs(Graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200901\n",
      "9.0 km\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/modules/rhel8/conda/install/envs/production-08-2024/lib/python3.9/site-packages/pyproj/crs/crs.py:1282: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n",
      "  proj = self._crs.to_proj4(version=version)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable not found: SNOWTEMP9_ga\n",
      "Variable not found: SNOWLIQ9_ga\n",
      "/lustre/storeB/immutable/archive/projects/remotesensing/satellite/2020/09/01/amsr2/jaxa/GW1AM2_202009010027_185B_L1SNBTBR_2220220.h5\n",
      "/lustre/storeB/immutable/archive/projects/remotesensing/satellite/2020/09/01/amsr2/jaxa/GW1AM2_202009010206_201B_L1SNBTBR_2220220.h5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create dataset (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [49], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m MEPS_data \u001b[38;5;241m=\u001b[39m get_MEPS_data(date_task, MEPS_leadtime, MEPS_dim_variables, MEPS_PL_variables, Surfex_coord, crs, paths)()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m extract_graphs(date_task \u001b[38;5;241m=\u001b[39m date_task, \n\u001b[1;32m     24\u001b[0m                paths \u001b[38;5;241m=\u001b[39m paths, \n\u001b[1;32m     25\u001b[0m                AMSR2_task_frequency \u001b[38;5;241m=\u001b[39m AMSR2_task_frequency, \n\u001b[1;32m     26\u001b[0m                static_dimension \u001b[38;5;241m=\u001b[39m static_dimension,\n\u001b[1;32m     27\u001b[0m                crs \u001b[38;5;241m=\u001b[39m crs,\n\u001b[1;32m     28\u001b[0m                Surfex_coord \u001b[38;5;241m=\u001b[39m Surfex_coord, \n\u001b[1;32m     29\u001b[0m                Surfex_data \u001b[38;5;241m=\u001b[39m Surfex_data,\n\u001b[1;32m     30\u001b[0m                MEPS_data \u001b[38;5;241m=\u001b[39m MEPS_data)()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     32\u001b[0m tf \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn [48], line 107\u001b[0m, in \u001b[0;36mextract_graphs.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m Distances_to_footprint_center \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_distances(AMSR2_surfex_domain)\n\u001b[1;32m    106\u001b[0m Graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_graph_data(Distances_to_footprint_center, AMSR2_surfex_domain)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_graphs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGraphs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [48], line 94\u001b[0m, in \u001b[0;36mextract_graphs.write_graphs\u001b[0;34m(self, Graphs)\u001b[0m\n\u001b[1;32m     92\u001b[0m         hdf\u001b[38;5;241m.\u001b[39mcreate_dataset(var, data \u001b[38;5;241m=\u001b[39m var_conc) \n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[43mhdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mGraphs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/modules/rhel8/conda/install/envs/production-08-2024/lib/python3.9/site-packages/h5py/_hl/group.py:161\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    158\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    159\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 161\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m/modules/rhel8/conda/install/envs/production-08-2024/lib/python3.9/site-packages/h5py/_hl/dataset.py:156\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     sid \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(shape, maxshape)\n\u001b[0;32m--> 156\u001b[0m dset_id \u001b[38;5;241m=\u001b[39m \u001b[43mh5d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdcpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[1;32m    159\u001b[0m     dset_id\u001b[38;5;241m.\u001b[39mwrite(h5s\u001b[38;5;241m.\u001b[39mALL, h5s\u001b[38;5;241m.\u001b[39mALL, data)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5d.pyx:87\u001b[0m, in \u001b[0;36mh5py.h5d.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create dataset (name already exists)"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "#\n",
    "list_dates = make_list_dates(date_min, date_max)\n",
    "date_task = list_dates[SGE_TASK_ID - 1]\n",
    "date_task_hours_AMSR2 = date_task + hours_AMSR2\n",
    "AMSR2_task_radius = AMSR2_footprint_radius[AMSR2_frequencies.index(AMSR2_task_frequency)]\n",
    "static_dimension = int(2 * 1000 * AMSR2_task_radius / (np.sqrt(MEPS_spatial_resolution ** 2 + MEPS_spatial_resolution ** 2))) ** 2\n",
    "print(date_task)\n",
    "print(str(AMSR2_task_radius) + \" km\")\n",
    "#\n",
    "Surfex_coord, Surfex_PGD = get_surfex_coordinates(crs, surfex_PGD_variables, paths)()\n",
    "Surfex_data = read_surfex_data(date_task_hours_AMSR2, paths, predictor_variables)\n",
    "Surfex_data[\"WSN_T_ISBA\"] = calculate_WSN_T_ISBA(Surfex_data, n_soil_layers)\n",
    "Surfex_data[\"FRAC_LAND_AND_SEA_WATER\"] = Surfex_data[\"FRAC_WATER\"] + Surfex_data[\"FRAC_SEA\"] \n",
    "Surfex_data[\"SNOW_GRADIENT\"] = (Surfex_data[\"SNOWTEMP12_ga\"] - Surfex_data[\"SNOWTEMP1_ga\"]) / Surfex_data[\"DSN_T_ISBA\"]\n",
    "Surfex_data[\"SNOW_GRADIENT\"][Surfex_data[\"SNOW_GRADIENT\"] > 50] = 50\n",
    "Surfex_data[\"SNOW_GRADIENT\"][Surfex_data[\"SNOW_GRADIENT\"] < -50] = -50\n",
    "for var in Surfex_PGD:\n",
    "    Surfex_data[var] = Surfex_PGD[var]\n",
    "#\n",
    "MEPS_data = get_MEPS_data(date_task, MEPS_leadtime, MEPS_dim_variables, MEPS_PL_variables, Surfex_coord, crs, paths)()\n",
    "#\n",
    "extract_graphs(date_task = date_task, \n",
    "               paths = paths, \n",
    "               AMSR2_task_frequency = AMSR2_task_frequency, \n",
    "               static_dimension = static_dimension,\n",
    "               crs = crs,\n",
    "               Surfex_coord = Surfex_coord, \n",
    "               Surfex_data = Surfex_data,\n",
    "               MEPS_data = MEPS_data)()\n",
    "#\n",
    "tf = time.time()\n",
    "print(\"Computing time: \", tf - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "production-08-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
