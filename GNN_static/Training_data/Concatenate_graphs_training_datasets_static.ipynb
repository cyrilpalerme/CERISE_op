{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGE_TASK_ID = 4\n",
    "#\n",
    "date_min = \"20200901\"\n",
    "date_max = \"20200905\"\n",
    "subsampling = 100\n",
    "#\n",
    "paths = {}\n",
    "paths[\"daily_data\"] = \"/lustre/storeB/project/nwp/H2O/wp3/Deep_learning_predictions/Training_data_GNN/\"\n",
    "#\n",
    "AMSR2_frequencies = [\"6.9\", \"7.3\", \"10.7\", \"18.7\", \"23.8\", \"36.5\"]\n",
    "AMSR2_frequency_task = AMSR2_frequencies[SGE_TASK_ID - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list_dates(date_min, date_max):\n",
    "    current_date = datetime.datetime.strptime(date_min, \"%Y%m%d\")\n",
    "    end_date = datetime.datetime.strptime(date_max, \"%Y%m%d\")\n",
    "    list_dates = []\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime(\"%Y%m%d\")\n",
    "        list_dates.append(date_str)\n",
    "        current_date = current_date + datetime.timedelta(days = 1)\n",
    "    return(list_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class concatenate_graphs_from_multiple_dates():\n",
    "    def __init__(self, N_Graph_IDs, date_task, date_min, date_max, subsampling, AMSR2_frequency_task, paths):\n",
    "        self.N_Graph_IDs = N_Graph_IDs\n",
    "        self.date_task = date_task\n",
    "        self.date_min = date_min\n",
    "        self.date_max = date_max\n",
    "        self.subsampling = subsampling\n",
    "        self.AMSR2_frequency_task = AMSR2_frequency_task\n",
    "        self.paths = paths\n",
    "    #\n",
    "    def concatenate_graphs(self):\n",
    "        path_data = self.paths[\"daily_data\"] + self.AMSR2_frequency_task.split('.')[0] + \"GHz_static/\" + self.date_task[0:4] + \"/\" + self.date_task[4:6] + \"/\"\n",
    "        filename_data = path_data + \"Graphs_\" + self.date_task + \".h5\"\n",
    "        filename_output = self.paths[\"daily_data\"] + self.AMSR2_frequency_task.split('.')[0] + \"GHz_static/Graphs_\" + self.date_min + \"_\" + self.date_max + \"_subsampling_\" + str(self.subsampling) + \".h5\"\n",
    "        #\n",
    "        with h5py.File(filename_output, \"a\") as hdf_output:\n",
    "            if os.path.isfile(filename_data) == True:\n",
    "                with h5py.File(filename_data, \"r\") as hdf_data:\n",
    "                    for var in hdf_data.keys():\n",
    "                        var_data = hdf_data[var][()]\n",
    "                        if var not in hdf_output:\n",
    "                            if var_data.ndim == 1:\n",
    "                                maxshape = (None,)\n",
    "                            elif var_data.ndim == 2:\n",
    "                                maxshape = (None, var_data.shape[1])\n",
    "                            elif var_data.ndim == 3:\n",
    "                                maxshape = (None, var_data.shape[1], var_data.shape[2])\n",
    "                            hdf_output.create_dataset(var, data = var_data, maxshape = maxshape)\n",
    "                        else:\n",
    "                            var_data_output = hdf_output[var]\n",
    "                            if var_data_output.ndim == 1:\n",
    "                                var_data_output.resize((var_data_output.shape[0] + var_data.shape[0]), axis = 0)\n",
    "                            elif var_data_output.ndim == 2:\n",
    "                                var_data_output.resize((var_data_output.shape[0] + var_data.shape[0]), axis = 0)\n",
    "                            elif var_data_output.ndim == 3:\n",
    "                                var_data_output.resize((var_data_output.shape[0] + var_data.shape[0]), axis = 0)\n",
    "                            var_data_output[-var_data.shape[0]:] = var_data\n",
    "    #\n",
    "    def __call__(self):\n",
    "        Graph_ID_output = self.concatenate_graphs()\n",
    "        return(Graph_ID_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200901\n",
      "20200902\n",
      "20200903\n",
      "20200904\n",
      "20200905\n",
      "None <class 'NoneType'>\n",
      "Computing time:  12.683761835098267\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "N_Graph_ID_cum = 0\n",
    "list_dates = make_list_dates(date_min, date_max)\n",
    "for di, date_task in enumerate(list_dates):\n",
    "    print(date_task)\n",
    "    #try:\n",
    "    N_Graph_ID_output = concatenate_graphs_from_multiple_dates(N_Graph_IDs = N_Graph_ID_cum, \n",
    "                                                                date_task = date_task, \n",
    "                                                                date_min = date_min, \n",
    "                                                                date_max = date_max, \n",
    "                                                                subsampling = subsampling,\n",
    "                                                                AMSR2_frequency_task = AMSR2_frequency_task, \n",
    "                                                                paths = paths)()\n",
    "    N_Graph_ID_cum = N_Graph_ID_output\n",
    "    #except:\n",
    "    #    pass\n",
    "#\n",
    "print(N_Graph_ID_cum, type(N_Graph_ID_cum))\n",
    "tf = time.time()\n",
    "print(\"Computing time: \", tf - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "production-08-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
