{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e07f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import h5py\n",
    "import netCDF4\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "#\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "#print(\"GPUs available: \", tf.config.list_physical_devices('GPU'))\n",
    "#physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#print(physical_devices)\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b15a38",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce180345",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CNN_v2\"\n",
    "experiment = \"v0\"\n",
    "stride = \"1\"\n",
    "#\n",
    "function_path = \"/lustre/storeB/users/cyrilp/CERISE/Scripts/Patch_CNN/Models/\" + model_name + \"/\"\n",
    "sys.path.insert(0, function_path)\n",
    "from CNN import *\n",
    "#\n",
    "date_min_test = \"20220901\"\n",
    "date_max_test = \"20230601\"\n",
    "hours_AMSR2 = \"H03\"\n",
    "domain_size = (250, 250)\n",
    "stride_prediction = 1\n",
    "#\n",
    "paths = {}\n",
    "paths[\"base\"] = \"/lustre/storeB/project/nwp/H2O/wp3/Deep_learning_predictions/\"\n",
    "paths[\"training\"] = paths[\"base\"] + \"Training_data/\"\n",
    "paths[\"model\"] = paths[\"base\"] + \"Patches/Models/\" + model_name + \"/\" + experiment + \"/\"\n",
    "paths[\"normalization_stats\"] = paths[\"base\"] + \"Normalization/\"\n",
    "paths[\"predictions_netCDF\"] = paths[\"base\"] + \"Patches/Predictions/\" + model_name + \"/netCDF/\" + experiment + \"_stride_\" + stride + \"/\"\n",
    "paths[\"prediction_scores\"] = paths[\"base\"] + \"Patches/Predictions/\" + model_name + \"/score/\"\n",
    "#\n",
    "for var in paths:\n",
    "    if os.path.isdir(paths[var]) == False:\n",
    "        os.system(\"mkdir -p \" + paths[var])\n",
    "#\n",
    "file_normalization = paths[\"normalization_stats\"] + \"Stats_normalization_20200901_20220531.h5\"\n",
    "file_model_weights = paths[\"model\"] + model_name + \".h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f968ac",
   "metadata": {},
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b38421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_targets = [\"tb18_v\", \"tb36_v\"]\n",
    "#\n",
    "predictors = {}\n",
    "predictors[\"constants\"] = [\"ZS\", \"PATCHP1\", \"PATCHP2\", \"FRAC_LAND_AND_SEA_WATER\"]\n",
    "predictors[\"ISBA\"] = [\"Q2M_ISBA\", \"DSN_T_ISBA\", \"WSN_T_ISBA\", \"LAI_ga\", \"TS_ISBA\", \"PSN_ISBA\"]\n",
    "predictors[\"TG\"] = [1, 2]\n",
    "predictors[\"WG\"] = [1, 2]\n",
    "predictors[\"WGI\"] = [1, 2]\n",
    "predictors[\"WSN_VEG\"] = [1, 2]\n",
    "predictors[\"RSN_VEG\"] = [1, 2]\n",
    "predictors[\"HSN_VEG\"] = [1, 2]\n",
    "predictors[\"SNOWTEMP\"] = [1, 12]\n",
    "predictors[\"SNOWLIQ\"] = [1, 12]\n",
    "#\n",
    "list_predictors = predictors[\"constants\"] + predictors[\"ISBA\"]\n",
    "for pred in predictors:\n",
    "    if (pred != \"constants\") and (pred != \"ISBA\"):\n",
    "        for lay in predictors[pred]:\n",
    "            var_name = pred + str(lay) + \"_ga\"\n",
    "            list_predictors = list_predictors + [var_name]\n",
    "#\n",
    "model_params = {\"list_predictors\": list_predictors,\n",
    "                \"list_targets\": list_targets, \n",
    "                \"patch_dim\": (5, 5),\n",
    "                \"batch_size\": 512,\n",
    "                \"conv_filters\": [32, 64, 128, 64],\n",
    "                \"dense_width\": [32, 16],\n",
    "                \"activation\": \"relu\",\n",
    "                \"kernel_initializer\": \"he_normal\",\n",
    "                \"batch_norm\": True,\n",
    "                \"pooling_type\": \"Average\",\n",
    "                \"dropout\": 0,\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef89e67",
   "metadata": {},
   "source": [
    "# Make list dates function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "742796d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list_dates(date_min, date_max):\n",
    "    current_date = datetime.datetime.strptime(date_min, \"%Y%m%d\")\n",
    "    end_date = datetime.datetime.strptime(date_max, \"%Y%m%d\")\n",
    "    list_dates = []\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime(\"%Y%m%d\")\n",
    "        filename = paths[\"training\"] + date_str[0:4] + \"/\" + date_str[4:6] + \"/\" + \"Dataset_\" + date_str + hours_AMSR2 + \".nc\"\n",
    "        if os.path.isfile(filename):\n",
    "            list_dates.append(date_str)\n",
    "        current_date = current_date + datetime.timedelta(days = 1)\n",
    "    return(list_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6e81cf",
   "metadata": {},
   "source": [
    "# Standardization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57ce336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_normalization_stats(file_normalization):\n",
    "    normalization_stats = {}\n",
    "    hf = h5py.File(file_normalization, \"r\")\n",
    "    for var in hf:\n",
    "        normalization_stats[var] = hf[var][()]\n",
    "    hf.close()\n",
    "    return(normalization_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08999c7e",
   "metadata": {},
   "source": [
    "# Extract evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7622fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_eval_data(date_task, list_targets, hours_AMSR2, paths):\n",
    "    Eval_data = {}\n",
    "    filename_training = paths[\"training\"] + date_task[0:4] + \"/\" + date_task[4:6] + \"/\" + \"Dataset_\" + date_task + hours_AMSR2 + \".nc\"\n",
    "    nc = netCDF4.Dataset(filename_training)\n",
    "    for var in list_targets:\n",
    "        var_data = nc.variables[var][:,:]\n",
    "        if np.shape(var_data.mask) == np.shape(var_data):\n",
    "            var_data[var_data.mask == True] = np.nan\n",
    "        Eval_data[var] = np.copy(var_data)\n",
    "    #\n",
    "    Eval_data[\"FRAC_SEA\"] = nc.variables[\"FRAC_SEA\"][:,:]\n",
    "    Eval_data[\"FRAC_WATER\"] = nc.variables[\"FRAC_WATER\"][:,:]\n",
    "    nc.close()\n",
    "    return(Eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a98551",
   "metadata": {},
   "source": [
    "# Make predictions function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b92d6120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_predictions():\n",
    "    def __init__(self, date_task, model, model_params, normalization_stats, paths, hours_AMSR2, domain_size, stride_prediction):\n",
    "        self.date_task = date_task\n",
    "        self.model = model\n",
    "        self.model_params = model_params\n",
    "        self.normalization_stats = normalization_stats\n",
    "        self.paths = paths\n",
    "        self.hours_AMSR2 = hours_AMSR2\n",
    "        self.domain_size = domain_size\n",
    "        self.stride_prediction = stride_prediction\n",
    "        self.patch_dim = model_params[\"patch_dim\"][0]\n",
    "    #\n",
    "    def normalize(self, var, var_data):\n",
    "        norm_var_data = (var_data - self.normalization_stats[var + \"_min\"]) / (self.normalization_stats[var + \"_max\"] - self.normalization_stats[var + \"_min\"])\n",
    "        return(norm_var_data)\n",
    "    #\n",
    "    def unnormalize(self, var, norm_var_data):\n",
    "        unnorm_var = norm_var_data * (self.normalization_stats[var + \"_max\"] - self.normalization_stats[var + \"_min\"]) + self.normalization_stats[var + \"_min\"]\n",
    "        return(unnorm_var)\n",
    "    #\n",
    "    def load_data_full_grid(self):\n",
    "        X = np.full((1, *self.domain_size, len(self.model_params[\"list_predictors\"])), np.nan)\n",
    "        filename = self.paths[\"training\"] + self.date_task[0:4] + \"/\" + self.date_task[4:6] + \"/\" + \"Dataset_\" + self.date_task + self.hours_AMSR2 + \".nc\"\n",
    "        nc = netCDF4.Dataset(filename, \"r\")\n",
    "        for v, var in enumerate(self.model_params[\"list_predictors\"]):\n",
    "            var_data = nc.variables[var][:,:]\n",
    "            if np.shape(var_data.mask) == np.shape(var_data):\n",
    "                var_data[var_data.mask == True] = self.normalization_stats[var + \"_min\"]\n",
    "            var_data[np.isnan(var_data) == True] = self.normalization_stats[var + \"_min\"]\n",
    "            X[0,:,:,v] = self.normalize(var, var_data)\n",
    "        nc.close()\n",
    "        return(X)\n",
    "    #\n",
    "    def make_patches(self, dataset):\n",
    "        # dataset should have the following shape: (n_samples (1), y_dim, x_dim, n_variables)\n",
    "        n_samples, y_dim, x_dim, num_vars = np.shape(dataset)\n",
    "        # Calculate the shape of the new array\n",
    "        new_shape = (1,\n",
    "                    (y_dim - self.patch_dim) // self.stride_prediction + 1,\n",
    "                    (x_dim - self.patch_dim) // self.stride_prediction + 1,\n",
    "                    self.patch_dim,\n",
    "                    self.patch_dim,\n",
    "                    num_vars)\n",
    "        # Calculate the strides of the new array\n",
    "        new_strides = (dataset.strides[0],\n",
    "                       dataset.strides[1] * self.stride_prediction,\n",
    "                       dataset.strides[2] * self.stride_prediction,\n",
    "                       dataset.strides[1],\n",
    "                       dataset.strides[2],\n",
    "                       dataset.strides[3])\n",
    "        # Use as_strided to create the new array\n",
    "        patches = np.squeeze(as_strided(dataset, shape = new_shape, strides = new_strides), axis = 0)\n",
    "        patches = patches.reshape(-1, self.patch_dim, self.patch_dim, num_vars)\n",
    "        return(patches)\n",
    "    #\n",
    "    def grid_patch_prediction(self, patch_predictions):\n",
    "        idx = 0\n",
    "        sum_pred = np.zeros(self.domain_size)\n",
    "        N_pred = np.zeros(self.domain_size)\n",
    "        #\n",
    "        for i in range(self.domain_size[0] - self.patch_dim + 1):\n",
    "            for j in range(self.domain_size[1] - self.patch_dim + 1):\n",
    "                for di in range(self.patch_dim):\n",
    "                    for dj in range(self.patch_dim):\n",
    "                        sum_pred[i + di, j + dj] = sum_pred[i + di, j + dj] + patch_predictions[idx]\n",
    "                        N_pred[i + di, j + dj] = N_pred[i + di, j + dj] + 1\n",
    "                idx = idx + 1\n",
    "        N_pred[N_pred == 0] = 1\n",
    "        Pred = sum_pred / N_pred\n",
    "        return(Pred)\n",
    "    #\n",
    "    def make_prediction(self):\n",
    "        X_full_grid = self.load_data_full_grid()\n",
    "        X_patches = self.make_patches(X_full_grid)\n",
    "        #\n",
    "        model_predictions = np.squeeze(self.model.predict(X_patches))\n",
    "        Pred = {}\n",
    "        for v, var in enumerate(self.model_params[\"list_targets\"]):\n",
    "            if len(self.model_params[\"list_targets\"]) == 1:\n",
    "                var_data = self.unnormalize(var, model_predictions[:])\n",
    "            else:\n",
    "                var_data = self.unnormalize(var, model_predictions[:,v])\n",
    "            #\n",
    "            Pred[var] = self.grid_patch_prediction(var_data)\n",
    "        #\n",
    "        return(Pred)\n",
    "    #\n",
    "    def __call__(self):\n",
    "        Predictions = self.make_prediction()\n",
    "        return(Predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa469fe",
   "metadata": {},
   "source": [
    "# Save predictions in netCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77a619f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_in_netCDF(date_task, Pred_data, Eval_data, list_targets, paths, stride_prediction):\n",
    "    file_output = paths[\"predictions_netCDF\"] + \"Predictions_\" + date_task + \".nc\"\n",
    "    output_netcdf = netCDF4.Dataset(file_output, \"w\", format = \"NETCDF4\")\n",
    "    Outputs = vars()\n",
    "    #\n",
    "    dimensions = [\"x\", \"y\"]\n",
    "    for di in dimensions:\n",
    "        Outputs[di] = output_netcdf.createDimension(di, 250)\n",
    "    #\n",
    "    for var in list_targets:\n",
    "        Outputs[\"target_\" + var] = output_netcdf.createVariable(\"target_\" + var, \"d\", (\"y\", \"x\"))\n",
    "        Outputs[\"target_\" + var][:,:] = Eval_data[var]\n",
    "    #\n",
    "    for var in Pred_data:\n",
    "        Outputs[\"pred_\" + var] = output_netcdf.createVariable(\"pred_\" + var, \"d\", (\"y\", \"x\"))\n",
    "        Outputs[\"pred_\" + var][:,:] = Pred_data[var]\n",
    "    #\n",
    "    output_netcdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ee1e32",
   "metadata": {},
   "source": [
    "# Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee9c3dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class verification():\n",
    "    def __init__(self, date_task, date_min, date_max, Pred_data, Eval_data, paths, experiment, stride):\n",
    "        self.date_task = date_task\n",
    "        self.date_min = date_min\n",
    "        self.date_max = date_max\n",
    "        self.Pred_data = Pred_data\n",
    "        self.Eval_data = Eval_data\n",
    "        self.paths = paths\n",
    "        self.experiment = experiment\n",
    "        self.stride = stride\n",
    "    #\n",
    "    def Calculate_scores(self):\n",
    "        Scores = {}\n",
    "        for var in Pred_data:\n",
    "            Scores[\"MAE_\" + var] = np.nanmean(np.abs(self.Pred_data[var] - self.Eval_data[var]))\n",
    "            Scores[\"RMSE_\" + var] = np.sqrt(np.nanmean(np.square(self.Pred_data[var] - self.Eval_data[var])))\n",
    "            #\n",
    "            idx_land = self.Eval_data[\"FRAC_SEA\"] == 0\n",
    "            idx_land_excl_lakes = (self.Eval_data[\"FRAC_SEA\"] + self.Eval_data[\"FRAC_WATER\"] < 0.5)\n",
    "            Scores[\"Land_MAE_\" + var] = np.nanmean(np.abs(self.Pred_data[var][idx_land == True] - self.Eval_data[var][idx_land == True]))\n",
    "            Scores[\"Land_RMSE_\" + var] = np.sqrt(np.nanmean(np.square(self.Pred_data[var][idx_land == True] - self.Eval_data[var][idx_land == True])))\n",
    "            Scores[\"Land_excl_lakes_MAE_\" + var] = np.nanmean(np.abs(self.Pred_data[var][idx_land_excl_lakes == True] - self.Eval_data[var][idx_land_excl_lakes == True]))\n",
    "            Scores[\"Land_excl_lakes_RMSE_\" + var] = np.sqrt(np.nanmean(np.square(self.Pred_data[var][idx_land_excl_lakes == True] - self.Eval_data[var][idx_land_excl_lakes == True])))\n",
    "        return(Scores)\n",
    "    #\n",
    "    def save_scores(self, Scores):\n",
    "        output_file = self.paths[\"prediction_scores\"] + \"Scores_\" + self.date_min + \"_\" + self.date_max + \"_\" + self.experiment + \"_stride_\" + self.stride + \".txt\"\n",
    "        #\n",
    "        if self.date_task == self.date_min:\n",
    "            if os.path.isfile(output_file) == True:\n",
    "                os.system(\"rm \" + output_file)\n",
    "            #\n",
    "            header = \"date\"\n",
    "            for var in Scores:\n",
    "                header = header + \"\\t\" + var\n",
    "            #\n",
    "            output = open(output_file, \"a\")\n",
    "            output.write(header + \"\\n\")\n",
    "            output.close()\n",
    "        #\n",
    "        scores_str = self.date_task\n",
    "        for var in Scores:\n",
    "            scores_str = scores_str + \"\\t\" + str(Scores[var])\n",
    "        #\n",
    "        output = open(output_file, \"a\")\n",
    "        output.write(scores_str + \"\\n\")\n",
    "        output.close()\n",
    "    #\n",
    "    def __call__(self):\n",
    "        Scores = self.Calculate_scores()\n",
    "        self.save_scores(Scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccde903a",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "684e52f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220901\n",
      "P1 (50, 50, 5, 5, 35) [[0.7232849  0.73283996 0.73123046 0.72220233 0.72230551]\n",
      " [0.71930297 0.71529687 0.72682232 0.71799822 0.71167113]\n",
      " [0.71005099 0.70246856 0.70705348 0.69216373 0.70498625]\n",
      " [0.7105079  0.68812229 0.6796044  0.68037922 0.70346764]\n",
      " [0.71048405 0.69266144 0.69004477 0.69861974 0.70450266]]\n",
      "P2 (2500, 5, 5, 35) [[0.7232849  0.73283996 0.73123046 0.72220233 0.72230551]\n",
      " [0.71930297 0.71529687 0.72682232 0.71799822 0.71167113]\n",
      " [0.71005099 0.70246856 0.70705348 0.69216373 0.70498625]\n",
      " [0.7105079  0.68812229 0.6796044  0.68037922 0.70346764]\n",
      " [0.71048405 0.69266144 0.69004477 0.69861974 0.70450266]]\n",
      "shape X_patches (2500, 5, 5, 35) TS_ISBA\n",
      "...................\n",
      "[[0.7232849  0.73283996 0.73123046 0.72220233 0.72230551]\n",
      " [0.71930297 0.71529687 0.72682232 0.71799822 0.71167113]\n",
      " [0.71005099 0.70246856 0.70705348 0.69216373 0.70498625]\n",
      " [0.7105079  0.68812229 0.6796044  0.68037922 0.70346764]\n",
      " [0.71048405 0.69266144 0.69004477 0.69861974 0.70450266]] 25.000114483192643\n",
      "...................\n",
      "[[0.7232849  0.73283996 0.73123046 0.72220233 0.72230551]\n",
      " [0.71930297 0.71529687 0.72682232 0.71799822 0.71167113]\n",
      " [0.71005099 0.70246856 0.70705348 0.69216373 0.70498625]\n",
      " [0.7105079  0.68812229 0.6796044  0.68037922 0.70346764]\n",
      " [0.71048405 0.69266144 0.69004477 0.69861974 0.70450266]] 25.000114483192643\n",
      "model_predictions [0.7007 0.7007 0.7007 ... 0.7007 0.7007 0.7007] 0.7007 0.7007\n",
      "20220902\n",
      "P1 (50, 50, 5, 5, 35) [[0.74820313 0.76021278 0.75922053 0.73810267 0.73858565]\n",
      " [0.75257646 0.74409753 0.75241856 0.73987935 0.73487257]\n",
      " [0.74836252 0.73797854 0.73530322 0.73506081 0.72762536]\n",
      " [0.74474569 0.73011432 0.73757815 0.73293461 0.73638093]\n",
      " [0.74508015 0.73903394 0.73515937 0.74104749 0.74055362]]\n",
      "P2 (2500, 5, 5, 35) [[0.74820313 0.76021278 0.75922053 0.73810267 0.73858565]\n",
      " [0.75257646 0.74409753 0.75241856 0.73987935 0.73487257]\n",
      " [0.74836252 0.73797854 0.73530322 0.73506081 0.72762536]\n",
      " [0.74474569 0.73011432 0.73757815 0.73293461 0.73638093]\n",
      " [0.74508015 0.73903394 0.73515937 0.74104749 0.74055362]]\n",
      "shape X_patches (2500, 5, 5, 35) TS_ISBA\n",
      "...................\n",
      "[[0.74820313 0.76021278 0.75922053 0.73810267 0.73858565]\n",
      " [0.75257646 0.74409753 0.75241856 0.73987935 0.73487257]\n",
      " [0.74836252 0.73797854 0.73530322 0.73506081 0.72762536]\n",
      " [0.74474569 0.73011432 0.73757815 0.73293461 0.73638093]\n",
      " [0.74508015 0.73903394 0.73515937 0.74104749 0.74055362]] 25.000114483192643\n",
      "...................\n",
      "[[0.74820313 0.76021278 0.75922053 0.73810267 0.73858565]\n",
      " [0.75257646 0.74409753 0.75241856 0.73987935 0.73487257]\n",
      " [0.74836252 0.73797854 0.73530322 0.73506081 0.72762536]\n",
      " [0.74474569 0.73011432 0.73757815 0.73293461 0.73638093]\n",
      " [0.74508015 0.73903394 0.73515937 0.74104749 0.74055362]] 25.000114483192643\n",
      "model_predictions [0.7007 0.7007 0.7007 ... 0.7007 0.7007 0.7007] 0.7007 0.7007\n",
      "20220903\n",
      "P1 (50, 50, 5, 5, 35) [[0.74862387 0.75962338 0.76475221 0.76464087 0.75142509]\n",
      " [0.75386526 0.75193404 0.75892653 0.75001163 0.75428732]\n",
      " [0.75933734 0.74722366 0.75152519 0.75311649 0.75464462]\n",
      " [0.76432312 0.75342464 0.76553447 0.76068495 0.75270371]\n",
      " [0.77421953 0.77137961 0.76515523 0.75919255 0.76290359]]\n",
      "P2 (2500, 5, 5, 35) [[0.74862387 0.75962338 0.76475221 0.76464087 0.75142509]\n",
      " [0.75386526 0.75193404 0.75892653 0.75001163 0.75428732]\n",
      " [0.75933734 0.74722366 0.75152519 0.75311649 0.75464462]\n",
      " [0.76432312 0.75342464 0.76553447 0.76068495 0.75270371]\n",
      " [0.77421953 0.77137961 0.76515523 0.75919255 0.76290359]]\n",
      "shape X_patches (2500, 5, 5, 35) TS_ISBA\n",
      "...................\n",
      "[[0.74862387 0.75962338 0.76475221 0.76464087 0.75142509]\n",
      " [0.75386526 0.75193404 0.75892653 0.75001163 0.75428732]\n",
      " [0.75933734 0.74722366 0.75152519 0.75311649 0.75464462]\n",
      " [0.76432312 0.75342464 0.76553447 0.76068495 0.75270371]\n",
      " [0.77421953 0.77137961 0.76515523 0.75919255 0.76290359]] 25.000114483192643\n",
      "...................\n",
      "[[0.74862387 0.75962338 0.76475221 0.76464087 0.75142509]\n",
      " [0.75386526 0.75193404 0.75892653 0.75001163 0.75428732]\n",
      " [0.75933734 0.74722366 0.75152519 0.75311649 0.75464462]\n",
      " [0.76432312 0.75342464 0.76553447 0.76068495 0.75270371]\n",
      " [0.77421953 0.77137961 0.76515523 0.75919255 0.76290359]] 25.000114483192643\n",
      "model_predictions [0.7007 0.7007 0.7007 ... 0.7007 0.7007 0.7007] 0.7007 0.7007\n",
      "Computing time:  5.70207953453064\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "#\n",
    "list_dates = make_list_dates(date_min_test, date_max_test)\n",
    "normalization_stats = load_normalization_stats(file_normalization)\n",
    "model = CNN(**model_params).make_model()\n",
    "model.load_weights(file_model_weights)\n",
    "for date_task in list_dates:\n",
    "    print(date_task)\n",
    "    Eval_data = extract_eval_data(date_task, list_targets, hours_AMSR2, paths)\n",
    "    Pred_data = make_predictions(date_task, model, model_params, normalization_stats, paths, hours_AMSR2, domain_size, stride_prediction)()\n",
    "    save_predictions_in_netCDF(date_task, Pred_data, Eval_data, list_targets, paths, stride_prediction)\n",
    "    verification(date_task, date_min_test, date_max_test, Pred_data, Eval_data, paths, experiment, stride)()\n",
    "#\n",
    "tf = time.time()\n",
    "print(\"Computing time: \", tf - t0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mycondaTF",
   "language": "python",
   "name": "mycondatf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
