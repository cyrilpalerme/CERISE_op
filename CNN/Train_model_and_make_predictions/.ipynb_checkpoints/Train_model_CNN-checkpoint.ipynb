{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "580234d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import h5py\n",
    "import pickle\n",
    "import netCDF4\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "print(\"GPUs available: \", tf.config.list_physical_devices('GPU'))\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "#\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba4f7c",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aac19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CNN\"\n",
    "experiment = \"v14_20_epochs\"\n",
    "function_path = \"/lustre/storeB/users/cyrilp/CERISE/Scripts/Patch_CNN/Models/\" + model_name + \"/\"\n",
    "sys.path.insert(0, function_path)\n",
    "from Data_generator import *\n",
    "from CNN import *\n",
    "#\n",
    "Number_of_samples_training = 1034319\n",
    "Number_of_samples_validation = 441461\n",
    "#\n",
    "paths = {}\n",
    "paths[\"base\"] = \"/lustre/storeB/project/nwp/H2O/wp3/Deep_learning_predictions/\"\n",
    "paths[\"training\"] = paths[\"base\"] + \"Patches/Training_patches/\"\n",
    "paths[\"validation\"] = paths[\"base\"] + \"Patches/Validation_patches/\" \n",
    "paths[\"normalization_stats\"] = paths[\"base\"] + \"Normalization/\"\n",
    "paths[\"checkpoints\"] = paths[\"base\"] + \"Patches/Models/\" + model_name + \"/\" + experiment +\"/Checkpoints/\"\n",
    "paths[\"output\"] = paths[\"base\"] + \"Patches/Models/\" + model_name + \"/\" + experiment + \"/\"\n",
    "#\n",
    "for var in paths:\n",
    "    if os.path.isdir(paths[var]) == False:\n",
    "        os.system(\"mkdir -p \" + paths[var])\n",
    "#\n",
    "file_normalization = paths[\"normalization_stats\"] + \"Stats_normalization_20200901_20220531.h5\"\n",
    "file_checkpoints = paths[\"checkpoints\"] + \"Checkpoints.h5\"\n",
    "if os.path.isfile(file_checkpoints) == True:\n",
    "    os.system(\"rm \" + file_checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ab9f12",
   "metadata": {},
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aab405",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_targets = [\"tb18_v\", \"tb36_v\"]\n",
    "#\n",
    "predictors = {}\n",
    "predictors[\"constants\"] = [\"ZS\", \"PATCHP1\", \"PATCHP2\", \"FRAC_LAND_AND_SEA_WATER\"]\n",
    "predictors[\"ISBA\"] = [\"Q2M_ISBA\", \"DSN_T_ISBA\", \"LAI_ga\", \"TS_ISBA\", \"PSN_ISBA\"]\n",
    "predictors[\"TG\"] = [1, 2]\n",
    "predictors[\"WG\"] = [1, 2]\n",
    "predictors[\"WGI\"] = [1, 2]\n",
    "predictors[\"WSN_VEG\"] = [1, 2]\n",
    "predictors[\"RSN_VEG\"] = [1, 2]\n",
    "predictors[\"HSN_VEG\"] = [1, 2]\n",
    "predictors[\"SNOWTEMP\"] = [1, 12]\n",
    "predictors[\"SNOWLIQ\"] = [1, 12]\n",
    "#\n",
    "list_predictors = predictors[\"constants\"] + predictors[\"ISBA\"]\n",
    "for pred in predictors:\n",
    "    if (pred != \"constants\") and (pred != \"ISBA\"):\n",
    "        for lay in predictors[pred]:\n",
    "            var_name = pred + str(lay) + \"_ga\"\n",
    "            list_predictors = list_predictors + [var_name]\n",
    "#\n",
    "model_params = {\"list_predictors\": list_predictors,\n",
    "                \"list_targets\": list_targets, \n",
    "                \"patch_dim\": (5, 5),\n",
    "                \"batch_size\": 512,\n",
    "                \"conv_filters\": [32, 64, 128, 64],\n",
    "                \"dense_width\": [32, 16],\n",
    "                \"activation\": \"relu\",\n",
    "                \"kernel_initializer\": \"he_normal\",\n",
    "                \"batch_norm\": True,\n",
    "                \"pooling_type\": \"Average\",\n",
    "                \"dropout\": 0,\n",
    "                }\n",
    "#\n",
    "compile_params = {\"initial_learning_rate\": 0.005, \n",
    "                  \"decay_steps\": 6630,\n",
    "                  \"decay_rate\": 0.5,\n",
    "                  \"staircase\": True,\n",
    "                  \"n_epochs\": 20,\n",
    "                  }\n",
    "#\n",
    "model_and_compile_params = {**model_params, **compile_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150297bf",
   "metadata": {},
   "source": [
    "# Save model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4227e4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_model_parameters\u001b[39m(model_and_compile_params, model_history, paths \u001b[38;5;241m=\u001b[39m paths):\n\u001b[1;32m      2\u001b[0m     file_model_parameters \u001b[38;5;241m=\u001b[39m paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel_parameters.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m     file_model_training_history \u001b[38;5;241m=\u001b[39m paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining_history.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'paths' is not defined"
     ]
    }
   ],
   "source": [
    "def save_model_parameters(model_and_compile_params, model_history, paths = paths):\n",
    "    file_model_parameters = paths[\"output\"] + \"Model_parameters.txt\"\n",
    "    file_model_training_history = paths[\"output\"] + \"Training_history.pkl\"\n",
    "    #\n",
    "    if os.path.isfile(file_model_parameters) == True:\n",
    "        os.system(\"rm \" + file_model_parameters)\n",
    "    if os.path.isfile(file_model_training_history) == True:\n",
    "        os.system(\"rm \" + file_model_training_history)\n",
    "    #\n",
    "    pickle.dump(model_history.history, open(file_model_training_history, \"wb\"))\n",
    "    with open(file_model_parameters, \"w\") as output_file:\n",
    "        output_file.write(json.dumps(model_and_compile_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c330bae",
   "metadata": {},
   "source": [
    "# Standardization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48033ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_normalization_stats(file_normalization):\n",
    "    normalization_stats = {}\n",
    "    hf = h5py.File(file_normalization, \"r\")\n",
    "    for var in hf:\n",
    "        normalization_stats[var] = hf[var][()]\n",
    "    hf.close()\n",
    "    return(normalization_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c476fd",
   "metadata": {},
   "source": [
    "# Loss function and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f0c1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    MSE = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    return(MSE)\n",
    "\n",
    "def RMSE(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    RMSE_score = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "    return(RMSE_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26cd03",
   "metadata": {},
   "source": [
    "# Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b7d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_stats = load_normalization_stats(file_normalization)\n",
    "list_train_IDs = np.arange(Number_of_samples_training) \n",
    "list_valid_IDs = np.arange(Number_of_samples_validation) \n",
    "#\n",
    "params_train = {\"list_predictors\": model_params[\"list_predictors\"],\n",
    "                \"list_targets\": model_params[\"list_targets\"],\n",
    "                \"list_IDs\": list_train_IDs,\n",
    "                \"normalization_stats\": normalization_stats,\n",
    "                \"batch_size\": model_params[\"batch_size\"],\n",
    "                \"path_data\": paths[\"training\"],\n",
    "                \"dim\": model_params[\"patch_dim\"],\n",
    "                \"shuffle\": False,\n",
    "                }\n",
    "#\n",
    "params_valid = {\"list_predictors\": model_params[\"list_predictors\"],\n",
    "                \"list_targets\": model_params[\"list_targets\"],\n",
    "                \"list_IDs\": list_valid_IDs,\n",
    "                \"normalization_stats\": normalization_stats,\n",
    "                \"batch_size\": model_params[\"batch_size\"],\n",
    "                \"path_data\": paths[\"validation\"],\n",
    "                \"dim\": model_params[\"patch_dim\"],\n",
    "                \"shuffle\": False,\n",
    "                }\n",
    "#\n",
    "train_generator = Data_generator(**params_train)\n",
    "valid_generator = Data_generator(**params_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6a9e4",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1e7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate = compile_params[\"initial_learning_rate\"],\n",
    "    decay_steps = compile_params[\"decay_steps\"],\n",
    "    decay_rate = compile_params[\"decay_rate\"],\n",
    "    staircase = compile_params[\"staircase\"])\n",
    "#\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\n",
    "loss = MSE_loss\n",
    "metrics = RMSE\n",
    "#\n",
    "model = CNN(**model_params).make_model()\n",
    "print(type(model))\n",
    "print(model.summary())\n",
    "model.compile(loss = loss, metrics = metrics, optimizer = opt)\n",
    "print(\"Model compiled\")\n",
    "#\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = file_checkpoints, save_weights_only = True, monitor = 'val_loss', mode = 'min', verbose = 2, save_best_only = True)\n",
    "#\n",
    "model_history = model.fit(train_generator, validation_data = valid_generator, epochs = compile_params[\"n_epochs\"], verbose = 2, callbacks = [checkpoint])\n",
    "print(\"Model fitted\")\n",
    "#\n",
    "filename_model = model_name + \".h5\"\n",
    "model.save_weights(paths[\"output\"] + filename_model)\n",
    "#\n",
    "save_model_parameters(model_and_compile_params, model_history, paths = paths)\n",
    "#\n",
    "t1 = time.time()\n",
    "dt = t1 - t0\n",
    "print(\"Computing time: \" + str(dt) + \" seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mycondaTF",
   "language": "python",
   "name": "mycondatf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
